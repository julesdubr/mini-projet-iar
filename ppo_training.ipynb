{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = make_vec_env(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2', n_envs=16)\n",
    "eval_env = make_vec_env(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    gae_lambda=0.98,\n",
    "    gamma=0.999,\n",
    "    n_epochs=4,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    "    tensorboard_log=f'./runs/ppo_lunar{\"continuous\" if continuous else \"\"}_tensorboard/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo',\n",
    "    log_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo',\n",
    "    eval_freq=1000,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/ppo_lunarcontinuous_tensorboard/PPO_1\n",
      "Eval num_timesteps=16000, episode_reward=-229.10 +/- 116.26\n",
      "Episode length: 112.70 +/- 24.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 113      |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -263     |\n",
      "| time/              |          |\n",
      "|    fps             | 2245     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-120.90 +/- 52.94\n",
      "Episode length: 71.30 +/- 12.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 71.3         |\n",
      "|    mean_reward          | -121         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058553885 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.000247    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.94e+03     |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | -0.00551     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.52e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | -172     |\n",
      "| time/              |          |\n",
      "|    fps             | 1607     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-118.88 +/- 74.32\n",
      "Episode length: 69.80 +/- 13.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 69.8         |\n",
      "|    mean_reward          | -119         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046882965 |\n",
      "|    clip_fraction        | 0.0439       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.00294     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19e+03     |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.0043      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.21e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -155     |\n",
      "| time/              |          |\n",
      "|    fps             | 1446     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-102.72 +/- 54.47\n",
      "Episode length: 77.90 +/- 15.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 77.9         |\n",
      "|    mean_reward          | -103         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059260456 |\n",
      "|    clip_fraction        | 0.0471       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.000396    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 525          |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -0.00433     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.51e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | -123     |\n",
      "| time/              |          |\n",
      "|    fps             | 1339     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-180.09 +/- 84.14\n",
      "Episode length: 98.60 +/- 26.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 98.6         |\n",
      "|    mean_reward          | -180         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049110507 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | -0.0023      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 413          |\n",
      "|    n_updates            | 16           |\n",
      "|    policy_gradient_loss | -0.000327    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 1.23e+03     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | -86.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 1289     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-117.75 +/- 67.12\n",
      "Episode length: 88.30 +/- 14.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 88.3        |\n",
      "|    mean_reward          | -118        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004899035 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | -0.000292   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 338         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 709         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 118      |\n",
      "|    ep_rew_mean     | -64.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 1249     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-107.75 +/- 65.69\n",
      "Episode length: 107.40 +/- 16.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 107        |\n",
      "|    mean_reward          | -108       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 112000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00436232 |\n",
      "|    clip_fraction        | 0.0354     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.89      |\n",
      "|    explained_variance   | 1.62e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 243        |\n",
      "|    n_updates            | 24         |\n",
      "|    policy_gradient_loss | -0.00319   |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 443        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 136      |\n",
      "|    ep_rew_mean     | -75.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 1208     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-99.37 +/- 79.57\n",
      "Episode length: 124.80 +/- 27.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 125         |\n",
      "|    mean_reward          | -99.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 128000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005193728 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | -0.000214   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 127         |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 404         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 150      |\n",
      "|    ep_rew_mean     | -54.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 1162     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-24.20 +/- 136.82\n",
      "Episode length: 165.80 +/- 49.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 166         |\n",
      "|    mean_reward          | -24.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 144000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006523941 |\n",
      "|    clip_fraction        | 0.0508      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | -5.44e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 286         |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | -0.00384    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 499         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 150      |\n",
      "|    ep_rew_mean     | -39.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 1150     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-46.04 +/- 108.40\n",
      "Episode length: 157.60 +/- 39.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 158          |\n",
      "|    mean_reward          | -46          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063888715 |\n",
      "|    clip_fraction        | 0.0451       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -2.26e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 151          |\n",
      "|    n_updates            | 36           |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 357          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 141      |\n",
      "|    ep_rew_mean     | -30      |\n",
      "| time/              |          |\n",
      "|    fps             | 1126     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-39.26 +/- 104.14\n",
      "Episode length: 176.10 +/- 51.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 176          |\n",
      "|    mean_reward          | -39.3        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063411673 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | -2.03e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 240          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 565          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 164      |\n",
      "|    ep_rew_mean     | -14.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 1101     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=101.19 +/- 173.67\n",
      "Episode length: 233.40 +/- 79.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 233          |\n",
      "|    mean_reward          | 101          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063717533 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -2.5e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 333          |\n",
      "|    n_updates            | 44           |\n",
      "|    policy_gradient_loss | -0.000324    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 550          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 181      |\n",
      "|    ep_rew_mean     | -18      |\n",
      "| time/              |          |\n",
      "|    fps             | 1035     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=34.19 +/- 175.65\n",
      "Episode length: 248.80 +/- 102.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 249         |\n",
      "|    mean_reward          | 34.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 208000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004680286 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | -3.46e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 254         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 611         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 227      |\n",
      "|    ep_rew_mean     | -12.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 998      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 213      |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=52.29 +/- 152.81\n",
      "Episode length: 249.20 +/- 75.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 249          |\n",
      "|    mean_reward          | 52.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050848844 |\n",
      "|    clip_fraction        | 0.0529       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | -4.77e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 263          |\n",
      "|    n_updates            | 52           |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 546          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 285      |\n",
      "|    ep_rew_mean     | -16.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 952      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=22.19 +/- 165.33\n",
      "Episode length: 271.40 +/- 75.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 271         |\n",
      "|    mean_reward          | 22.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005110978 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.91       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 406         |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 718         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 393      |\n",
      "|    ep_rew_mean     | -13.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 904      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=40.92 +/- 183.45\n",
      "Episode length: 271.80 +/- 55.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 272         |\n",
      "|    mean_reward          | 40.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004818445 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 263         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000848   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 452         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 442      |\n",
      "|    ep_rew_mean     | -1.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 865      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=100.79 +/- 160.16\n",
      "Episode length: 277.10 +/- 68.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 277         |\n",
      "|    mean_reward          | 101         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 272000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004098559 |\n",
      "|    clip_fraction        | 0.0413      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | -4.77e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 249         |\n",
      "|    n_updates            | 64          |\n",
      "|    policy_gradient_loss | 4.39e-05    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 496         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 487      |\n",
      "|    ep_rew_mean     | 19.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 836      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 333      |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=15.58 +/- 105.87\n",
      "Episode length: 221.90 +/- 98.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 222         |\n",
      "|    mean_reward          | 15.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 288000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005135549 |\n",
      "|    clip_fraction        | 0.049       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | -3.58e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 232         |\n",
      "|    n_updates            | 68          |\n",
      "|    policy_gradient_loss | -0.000261   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 497         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 515      |\n",
      "|    ep_rew_mean     | 28.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=82.19 +/- 170.51\n",
      "Episode length: 328.00 +/- 108.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | 82.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 304000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039833924 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 248          |\n",
      "|    n_updates            | 72           |\n",
      "|    policy_gradient_loss | -0.000428    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 402          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 576      |\n",
      "|    ep_rew_mean     | 36.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 790      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 393      |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=132.17 +/- 151.44\n",
      "Episode length: 341.70 +/- 83.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 342         |\n",
      "|    mean_reward          | 132         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002986539 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 4.77e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 153         |\n",
      "|    n_updates            | 76          |\n",
      "|    policy_gradient_loss | -0.000723   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 426         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 623      |\n",
      "|    ep_rew_mean     | 44.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 425      |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=163.08 +/- 111.21\n",
      "Episode length: 279.40 +/- 55.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 279         |\n",
      "|    mean_reward          | 163         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004721805 |\n",
      "|    clip_fraction        | 0.0405      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 4.32e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 123         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00235    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 244         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 680      |\n",
      "|    ep_rew_mean     | 52.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 752      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 457      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=167.61 +/- 120.51\n",
      "Episode length: 284.40 +/- 50.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 284         |\n",
      "|    mean_reward          | 168         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 352000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004422079 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 99.3        |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 192         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 685      |\n",
      "|    ep_rew_mean     | 52       |\n",
      "| time/              |          |\n",
      "|    fps             | 729      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 493      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=72.82 +/- 110.97\n",
      "Episode length: 248.60 +/- 72.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 249         |\n",
      "|    mean_reward          | 72.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 368000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004083596 |\n",
      "|    clip_fraction        | 0.0282      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 87.6        |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | -0.000888   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 279         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 707      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 716      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 526      |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=177.48 +/- 105.33\n",
      "Episode length: 293.50 +/- 72.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 294          |\n",
      "|    mean_reward          | 177          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 384000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022911248 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.1         |\n",
      "|    n_updates            | 92           |\n",
      "|    policy_gradient_loss | -0.000374    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 83.8         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 65.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 559      |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=130.46 +/- 144.95\n",
      "Episode length: 273.00 +/- 36.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 273          |\n",
      "|    mean_reward          | 130          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029881708 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.7         |\n",
      "|    n_updates            | 96           |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 81.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 72.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 694      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 589      |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=244.50 +/- 39.63\n",
      "Episode length: 325.30 +/- 88.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | 245          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 416000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052519385 |\n",
      "|    clip_fraction        | 0.0431       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.4         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 76.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 684      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 622      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=233.93 +/- 66.92\n",
      "Episode length: 279.50 +/- 30.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 280          |\n",
      "|    mean_reward          | 234          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 432000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039628195 |\n",
      "|    clip_fraction        | 0.0305       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 69.8         |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | -0.000506    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 122          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 80.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 677      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 652      |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=256.13 +/- 21.68\n",
      "Episode length: 320.40 +/- 90.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | 256          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 448000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028026532 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.945        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 65.2         |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -7.73e-05    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 74.5         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 89.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 670      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 684      |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=220.76 +/- 93.31\n",
      "Episode length: 346.50 +/- 135.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | 221          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 464000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040665655 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | -9.6e-05     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 55.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 93.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 662      |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 716      |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=236.74 +/- 84.38\n",
      "Episode length: 284.60 +/- 37.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 285          |\n",
      "|    mean_reward          | 237          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053265225 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.2         |\n",
      "|    n_updates            | 116          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 79.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 90.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 658      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 746      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=245.23 +/- 35.36\n",
      "Episode length: 326.40 +/- 33.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | 245          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 496000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040346794 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.953        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 73.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 96.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 779      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=246.01 +/- 31.43\n",
      "Episode length: 339.10 +/- 130.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | 246          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044418196 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | 0.968        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.91         |\n",
      "|    n_updates            | 124          |\n",
      "|    policy_gradient_loss | -0.000293    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 34.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 104      |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 812      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=235.92 +/- 26.70\n",
      "Episode length: 348.20 +/- 129.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 348          |\n",
      "|    mean_reward          | 236          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 528000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048378883 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.3         |\n",
      "|    n_updates            | 128          |\n",
      "|    policy_gradient_loss | -0.000301    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 42.1         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e6), callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'data/policies/LunarLanderf'LunarLander{\"Continuous\" if continuous else \"\"}-v2'-v2#pop#training')\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
