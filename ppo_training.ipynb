{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = make_vec_env(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2', n_envs=16)\n",
    "eval_env = gym.make(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    gae_lambda=0.98,\n",
    "    gamma=0.999,\n",
    "    n_epochs=4,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    "    tensorboard_log=f'./runs/ppo_lunar{\"continuous\" if continuous else \"\"}_tensorboard/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo',\n",
    "    log_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo',\n",
    "    eval_freq=1000,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/ppo_lunarcontinuous_tensorboard/PPO_1\n",
      "Eval num_timesteps=16000, episode_reward=-229.10 +/- 116.26\n",
      "Episode length: 112.70 +/- 24.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 113      |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -263     |\n",
      "| time/              |          |\n",
      "|    fps             | 2245     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-120.90 +/- 52.94\n",
      "Episode length: 71.30 +/- 12.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 71.3         |\n",
      "|    mean_reward          | -121         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058553885 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.000247    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.94e+03     |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | -0.00551     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.52e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | -172     |\n",
      "| time/              |          |\n",
      "|    fps             | 1607     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-118.88 +/- 74.32\n",
      "Episode length: 69.80 +/- 13.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 69.8         |\n",
      "|    mean_reward          | -119         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046882965 |\n",
      "|    clip_fraction        | 0.0439       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.00294     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19e+03     |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.0043      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.21e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -155     |\n",
      "| time/              |          |\n",
      "|    fps             | 1446     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-102.72 +/- 54.47\n",
      "Episode length: 77.90 +/- 15.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 77.9         |\n",
      "|    mean_reward          | -103         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059260456 |\n",
      "|    clip_fraction        | 0.0471       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.000396    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 525          |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -0.00433     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.51e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | -123     |\n",
      "| time/              |          |\n",
      "|    fps             | 1339     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-180.09 +/- 84.14\n",
      "Episode length: 98.60 +/- 26.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 98.6         |\n",
      "|    mean_reward          | -180         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049110507 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | -0.0023      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 413          |\n",
      "|    n_updates            | 16           |\n",
      "|    policy_gradient_loss | -0.000327    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 1.23e+03     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | -86.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 1289     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-117.75 +/- 67.12\n",
      "Episode length: 88.30 +/- 14.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 88.3        |\n",
      "|    mean_reward          | -118        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004899035 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | -0.000292   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 338         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 709         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 118      |\n",
      "|    ep_rew_mean     | -64.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 1249     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-107.75 +/- 65.69\n",
      "Episode length: 107.40 +/- 16.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 107        |\n",
      "|    mean_reward          | -108       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 112000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00436232 |\n",
      "|    clip_fraction        | 0.0354     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.89      |\n",
      "|    explained_variance   | 1.62e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 243        |\n",
      "|    n_updates            | 24         |\n",
      "|    policy_gradient_loss | -0.00319   |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 443        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 136      |\n",
      "|    ep_rew_mean     | -75.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 1208     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-99.37 +/- 79.57\n",
      "Episode length: 124.80 +/- 27.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 125         |\n",
      "|    mean_reward          | -99.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 128000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005193728 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | -0.000214   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 127         |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 404         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 150      |\n",
      "|    ep_rew_mean     | -54.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 1162     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-24.20 +/- 136.82\n",
      "Episode length: 165.80 +/- 49.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 166         |\n",
      "|    mean_reward          | -24.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 144000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006523941 |\n",
      "|    clip_fraction        | 0.0508      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | -5.44e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 286         |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | -0.00384    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 499         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 150      |\n",
      "|    ep_rew_mean     | -39.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 1150     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-46.04 +/- 108.40\n",
      "Episode length: 157.60 +/- 39.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 158          |\n",
      "|    mean_reward          | -46          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063888715 |\n",
      "|    clip_fraction        | 0.0451       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -2.26e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 151          |\n",
      "|    n_updates            | 36           |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 357          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 141      |\n",
      "|    ep_rew_mean     | -30      |\n",
      "| time/              |          |\n",
      "|    fps             | 1126     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-39.26 +/- 104.14\n",
      "Episode length: 176.10 +/- 51.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 176          |\n",
      "|    mean_reward          | -39.3        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063411673 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | -2.03e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 240          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 565          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 164      |\n",
      "|    ep_rew_mean     | -14.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 1101     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=101.19 +/- 173.67\n",
      "Episode length: 233.40 +/- 79.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 233          |\n",
      "|    mean_reward          | 101          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063717533 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -2.5e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 333          |\n",
      "|    n_updates            | 44           |\n",
      "|    policy_gradient_loss | -0.000324    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 550          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 181      |\n",
      "|    ep_rew_mean     | -18      |\n",
      "| time/              |          |\n",
      "|    fps             | 1035     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=34.19 +/- 175.65\n",
      "Episode length: 248.80 +/- 102.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 249         |\n",
      "|    mean_reward          | 34.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 208000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004680286 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | -3.46e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 254         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 611         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 227      |\n",
      "|    ep_rew_mean     | -12.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 998      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 213      |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=52.29 +/- 152.81\n",
      "Episode length: 249.20 +/- 75.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 249          |\n",
      "|    mean_reward          | 52.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050848844 |\n",
      "|    clip_fraction        | 0.0529       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | -4.77e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 263          |\n",
      "|    n_updates            | 52           |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 546          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 285      |\n",
      "|    ep_rew_mean     | -16.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 952      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=22.19 +/- 165.33\n",
      "Episode length: 271.40 +/- 75.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 271         |\n",
      "|    mean_reward          | 22.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005110978 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.91       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 406         |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 718         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 393      |\n",
      "|    ep_rew_mean     | -13.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 904      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=40.92 +/- 183.45\n",
      "Episode length: 271.80 +/- 55.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 272         |\n",
      "|    mean_reward          | 40.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004818445 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 263         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000848   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 452         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 442      |\n",
      "|    ep_rew_mean     | -1.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 865      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=100.79 +/- 160.16\n",
      "Episode length: 277.10 +/- 68.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 277         |\n",
      "|    mean_reward          | 101         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 272000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004098559 |\n",
      "|    clip_fraction        | 0.0413      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | -4.77e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 249         |\n",
      "|    n_updates            | 64          |\n",
      "|    policy_gradient_loss | 4.39e-05    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 496         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 487      |\n",
      "|    ep_rew_mean     | 19.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 836      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 333      |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=15.58 +/- 105.87\n",
      "Episode length: 221.90 +/- 98.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 222         |\n",
      "|    mean_reward          | 15.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 288000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005135549 |\n",
      "|    clip_fraction        | 0.049       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | -3.58e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 232         |\n",
      "|    n_updates            | 68          |\n",
      "|    policy_gradient_loss | -0.000261   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 497         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 515      |\n",
      "|    ep_rew_mean     | 28.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 812      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=82.19 +/- 170.51\n",
      "Episode length: 328.00 +/- 108.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | 82.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 304000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039833924 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 248          |\n",
      "|    n_updates            | 72           |\n",
      "|    policy_gradient_loss | -0.000428    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 402          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 576      |\n",
      "|    ep_rew_mean     | 36.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 790      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 393      |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=132.17 +/- 151.44\n",
      "Episode length: 341.70 +/- 83.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 342         |\n",
      "|    mean_reward          | 132         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002986539 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 4.77e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 153         |\n",
      "|    n_updates            | 76          |\n",
      "|    policy_gradient_loss | -0.000723   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 426         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 623      |\n",
      "|    ep_rew_mean     | 44.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 425      |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=163.08 +/- 111.21\n",
      "Episode length: 279.40 +/- 55.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 279         |\n",
      "|    mean_reward          | 163         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004721805 |\n",
      "|    clip_fraction        | 0.0405      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 4.32e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 123         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00235    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 244         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 680      |\n",
      "|    ep_rew_mean     | 52.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 752      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 457      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=167.61 +/- 120.51\n",
      "Episode length: 284.40 +/- 50.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 284         |\n",
      "|    mean_reward          | 168         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 352000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004422079 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 99.3        |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 192         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 685      |\n",
      "|    ep_rew_mean     | 52       |\n",
      "| time/              |          |\n",
      "|    fps             | 729      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 493      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=72.82 +/- 110.97\n",
      "Episode length: 248.60 +/- 72.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 249         |\n",
      "|    mean_reward          | 72.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 368000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004083596 |\n",
      "|    clip_fraction        | 0.0282      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 87.6        |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | -0.000888   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 279         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 707      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 716      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 526      |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=177.48 +/- 105.33\n",
      "Episode length: 293.50 +/- 72.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 294          |\n",
      "|    mean_reward          | 177          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 384000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022911248 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.1         |\n",
      "|    n_updates            | 92           |\n",
      "|    policy_gradient_loss | -0.000374    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 83.8         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 65.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 559      |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=130.46 +/- 144.95\n",
      "Episode length: 273.00 +/- 36.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 273          |\n",
      "|    mean_reward          | 130          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029881708 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.7         |\n",
      "|    n_updates            | 96           |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 81.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 72.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 694      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 589      |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=244.50 +/- 39.63\n",
      "Episode length: 325.30 +/- 88.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | 245          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 416000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052519385 |\n",
      "|    clip_fraction        | 0.0431       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.4         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 76.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 684      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 622      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=233.93 +/- 66.92\n",
      "Episode length: 279.50 +/- 30.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 280          |\n",
      "|    mean_reward          | 234          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 432000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039628195 |\n",
      "|    clip_fraction        | 0.0305       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 69.8         |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | -0.000506    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 122          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 80.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 677      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 652      |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=256.13 +/- 21.68\n",
      "Episode length: 320.40 +/- 90.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | 256          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 448000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028026532 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.945        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 65.2         |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -7.73e-05    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 74.5         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 89.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 670      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 684      |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=220.76 +/- 93.31\n",
      "Episode length: 346.50 +/- 135.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | 221          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 464000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040665655 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | -9.6e-05     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 55.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 93.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 662      |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 716      |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=236.74 +/- 84.38\n",
      "Episode length: 284.60 +/- 37.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 285          |\n",
      "|    mean_reward          | 237          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053265225 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.2         |\n",
      "|    n_updates            | 116          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 79.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 90.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 658      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 746      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=245.23 +/- 35.36\n",
      "Episode length: 326.40 +/- 33.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | 245          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 496000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040346794 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.953        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 73.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 96.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 779      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=246.01 +/- 31.43\n",
      "Episode length: 339.10 +/- 130.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | 246          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044418196 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | 0.968        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.91         |\n",
      "|    n_updates            | 124          |\n",
      "|    policy_gradient_loss | -0.000293    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 34.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 104      |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 812      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=235.92 +/- 26.70\n",
      "Episode length: 348.20 +/- 129.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 348          |\n",
      "|    mean_reward          | 236          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 528000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048378883 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.3         |\n",
      "|    n_updates            | 128          |\n",
      "|    policy_gradient_loss | -0.000301    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 42.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 843      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=241.97 +/- 34.00\n",
      "Episode length: 333.10 +/- 63.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 333         |\n",
      "|    mean_reward          | 242         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004439837 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 132         |\n",
      "|    policy_gradient_loss | -0.00072    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 25.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    fps             | 637      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 873      |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=242.39 +/- 83.96\n",
      "Episode length: 284.60 +/- 34.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 285          |\n",
      "|    mean_reward          | 242          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 560000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045322045 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 136          |\n",
      "|    policy_gradient_loss | -0.000795    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 38.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 114      |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 902      |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=254.98 +/- 23.05\n",
      "Episode length: 333.50 +/- 101.29\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 334          |\n",
      "|    mean_reward          | 255          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 576000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048772166 |\n",
      "|    clip_fraction        | 0.0529       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.32         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000907    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 17.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 934      |\n",
      "|    ep_rew_mean     | 122      |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 931      |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=232.54 +/- 82.86\n",
      "Episode length: 324.10 +/- 47.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | 233          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 592000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057518603 |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.978        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 144          |\n",
      "|    policy_gradient_loss | 0.00022      |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 26.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 124      |\n",
      "| time/              |          |\n",
      "|    fps             | 629      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 962      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=255.87 +/- 19.91\n",
      "Episode length: 310.30 +/- 39.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 310         |\n",
      "|    mean_reward          | 256         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 608000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005266057 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.3         |\n",
      "|    n_updates            | 148         |\n",
      "|    policy_gradient_loss | -0.000117   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 16.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 992      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=250.60 +/- 26.11\n",
      "Episode length: 298.00 +/- 25.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 298          |\n",
      "|    mean_reward          | 251          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051918305 |\n",
      "|    clip_fraction        | 0.0476       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.975        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.71         |\n",
      "|    n_updates            | 152          |\n",
      "|    policy_gradient_loss | -0.000701    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 36.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 941      |\n",
      "|    ep_rew_mean     | 131      |\n",
      "| time/              |          |\n",
      "|    fps             | 625      |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 1021     |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=242.44 +/- 30.31\n",
      "Episode length: 307.60 +/- 28.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 308         |\n",
      "|    mean_reward          | 242         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003588913 |\n",
      "|    clip_fraction        | 0.0495      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.5         |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 23.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 961      |\n",
      "|    ep_rew_mean     | 138      |\n",
      "| time/              |          |\n",
      "|    fps             | 624      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 1050     |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=260.55 +/- 13.97\n",
      "Episode length: 254.60 +/- 21.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 255          |\n",
      "|    mean_reward          | 261          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 656000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040378883 |\n",
      "|    clip_fraction        | 0.0372       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.977        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | 1.28e-05     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 33.9         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 970      |\n",
      "|    ep_rew_mean     | 141      |\n",
      "| time/              |          |\n",
      "|    fps             | 623      |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 1077     |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=225.58 +/- 81.23\n",
      "Episode length: 276.80 +/- 17.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 277          |\n",
      "|    mean_reward          | 226          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 672000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044308845 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.13         |\n",
      "|    n_updates            | 164          |\n",
      "|    policy_gradient_loss | 0.000442     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 8.09         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=207.81 +/- 116.72\n",
      "Episode length: 266.70 +/- 34.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 267      |\n",
      "|    mean_reward     | 208      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 978      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    fps             | 620      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 1109     |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=187.44 +/- 100.27\n",
      "Episode length: 291.40 +/- 36.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 291         |\n",
      "|    mean_reward          | 187         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 704000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004414692 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | 0.000281    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 12          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 978      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    fps             | 619      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 1137     |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=195.20 +/- 111.56\n",
      "Episode length: 299.50 +/- 44.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 300          |\n",
      "|    mean_reward          | 195          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 720000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047240057 |\n",
      "|    clip_fraction        | 0.0449       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.63         |\n",
      "|    n_updates            | 172          |\n",
      "|    policy_gradient_loss | 0.00049      |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 9.45         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 947      |\n",
      "|    ep_rew_mean     | 138      |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 1165     |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=207.16 +/- 77.26\n",
      "Episode length: 303.40 +/- 18.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 303          |\n",
      "|    mean_reward          | 207          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 736000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051298765 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | 0.956        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.7         |\n",
      "|    n_updates            | 176          |\n",
      "|    policy_gradient_loss | -0.000385    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 72           |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 949      |\n",
      "|    ep_rew_mean     | 140      |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 1196     |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=210.16 +/- 73.72\n",
      "Episode length: 299.20 +/- 26.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 299         |\n",
      "|    mean_reward          | 210         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 752000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005184691 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.08        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.000228    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 27          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 938      |\n",
      "|    ep_rew_mean     | 137      |\n",
      "| time/              |          |\n",
      "|    fps             | 614      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 1225     |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=213.00 +/- 83.02\n",
      "Episode length: 345.70 +/- 100.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | 213          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 768000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048855254 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.3         |\n",
      "|    n_updates            | 184          |\n",
      "|    policy_gradient_loss | 0.000674     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 26.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 938      |\n",
      "|    ep_rew_mean     | 134      |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 1255     |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=223.10 +/- 83.84\n",
      "Episode length: 289.70 +/- 32.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | 223          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 784000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041742856 |\n",
      "|    clip_fraction        | 0.0566       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.21         |\n",
      "|    n_updates            | 188          |\n",
      "|    policy_gradient_loss | 4.65e-05     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 7.1          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 126      |\n",
      "| time/              |          |\n",
      "|    fps             | 612      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 1284     |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=201.27 +/- 87.38\n",
      "Episode length: 281.70 +/- 20.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 282          |\n",
      "|    mean_reward          | 201          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 800000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025791726 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.941        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 96.9         |\n",
      "|    n_updates            | 192          |\n",
      "|    policy_gradient_loss | 0.000128     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 77.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 127      |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 1312     |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=246.78 +/- 19.84\n",
      "Episode length: 305.60 +/- 50.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | 247          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 816000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042332076 |\n",
      "|    clip_fraction        | 0.0532       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.976        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.37         |\n",
      "|    n_updates            | 196          |\n",
      "|    policy_gradient_loss | -0.000632    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 28.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    fps             | 609      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 1343     |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=228.33 +/- 77.82\n",
      "Episode length: 297.30 +/- 41.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 297         |\n",
      "|    mean_reward          | 228         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 832000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004371615 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.5        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.000696   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 81.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 124      |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 1372     |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=240.48 +/- 25.35\n",
      "Episode length: 312.80 +/- 13.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 313         |\n",
      "|    mean_reward          | 240         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 848000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004744046 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | 0.000539    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 42          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 124      |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 1401     |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=230.18 +/- 41.23\n",
      "Episode length: 382.70 +/- 112.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 383          |\n",
      "|    mean_reward          | 230          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 864000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041785063 |\n",
      "|    clip_fraction        | 0.0526       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.88         |\n",
      "|    n_updates            | 208          |\n",
      "|    policy_gradient_loss | -0.00067     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 9.54         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 1432     |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=179.50 +/- 118.84\n",
      "Episode length: 293.30 +/- 41.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 293        |\n",
      "|    mean_reward          | 179        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 880000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00487467 |\n",
      "|    clip_fraction        | 0.0539     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.89      |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.23       |\n",
      "|    n_updates            | 212        |\n",
      "|    policy_gradient_loss | -0.000398  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 6.31       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 926      |\n",
      "|    ep_rew_mean     | 127      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 1461     |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=246.84 +/- 15.47\n",
      "Episode length: 352.00 +/- 99.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 352         |\n",
      "|    mean_reward          | 247         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 896000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003299788 |\n",
      "|    clip_fraction        | 0.0398      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.79        |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.000223   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 40.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 130      |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 1492     |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=227.73 +/- 74.61\n",
      "Episode length: 296.60 +/- 29.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 297          |\n",
      "|    mean_reward          | 228          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 912000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036924484 |\n",
      "|    clip_fraction        | 0.0291       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.29         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000207    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 36.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 937      |\n",
      "|    ep_rew_mean     | 133      |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 1520     |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=263.59 +/- 12.59\n",
      "Episode length: 287.30 +/- 23.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 287          |\n",
      "|    mean_reward          | 264          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 928000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045555877 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 101          |\n",
      "|    n_updates            | 224          |\n",
      "|    policy_gradient_loss | 0.000267     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 39.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 131      |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 1549     |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=244.53 +/- 25.75\n",
      "Episode length: 305.10 +/- 23.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 305         |\n",
      "|    mean_reward          | 245         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 944000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005924862 |\n",
      "|    clip_fraction        | 0.0264      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.37        |\n",
      "|    n_updates            | 228         |\n",
      "|    policy_gradient_loss | -0.000385   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 77.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 129      |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 1577     |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=215.98 +/- 76.52\n",
      "Episode length: 295.20 +/- 16.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 295          |\n",
      "|    mean_reward          | 216          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 960000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043170927 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.975        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.07         |\n",
      "|    n_updates            | 232          |\n",
      "|    policy_gradient_loss | -0.000299    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 33.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 132      |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 1604     |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=224.57 +/- 73.44\n",
      "Episode length: 322.10 +/- 84.86\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 322       |\n",
      "|    mean_reward          | 225       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 976000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0054388 |\n",
      "|    clip_fraction        | 0.0578    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.83     |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.885     |\n",
      "|    n_updates            | 236       |\n",
      "|    policy_gradient_loss | 0.000294  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.54      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 129      |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 1632     |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=254.54 +/- 23.29\n",
      "Episode length: 345.90 +/- 143.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 346         |\n",
      "|    mean_reward          | 255         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 992000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004027742 |\n",
      "|    clip_fraction        | 0.0337      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 72.3        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 75.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 132      |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 1663     |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=259.97 +/- 10.76\n",
      "Episode length: 328.70 +/- 50.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 329          |\n",
      "|    mean_reward          | 260          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1008000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043974286 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.96         |\n",
      "|    n_updates            | 244          |\n",
      "|    policy_gradient_loss | -3.14e-07    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 7.69         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 136      |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 1692     |\n",
      "|    total_timesteps | 1015808  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1aff93737f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e6), callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'data/policies/LunarLander{\"Continuous\" if continuous else \"\"}-v2#ppo#training')\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a094b89469df4117159496550a75e1ef6dfce4ed77bce29eb93efb1f0342af31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
