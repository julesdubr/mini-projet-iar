{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = make_vec_env(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2', n_envs=16)\n",
    "eval_env = make_vec_env(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2', n_envs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# model = PPO(\n",
    "#     \"MlpPolicy\",\n",
    "#     train_env,\n",
    "#     n_steps=1024,\n",
    "#     batch_size=64,\n",
    "#     gae_lambda=0.98,\n",
    "#     gamma=0.999,\n",
    "#     n_epochs=4,\n",
    "#     ent_coef=0.01,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=f'./runs/ppo_lunar{\"continuous\" if continuous else \"\"}_tensorboard/'\n",
    "# )\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=f'./runs/ppo_opt_lunar{\"continuous\" if continuous else \"\"}_tensorboard/',\n",
    ")\n",
    "# model = PPO(\n",
    "#     \"MlpPolicy\",\n",
    "#     train_env,\n",
    "#     # learning_rate=3.9e-5,\n",
    "#     n_steps=2048,\n",
    "#     batch_size=256,\n",
    "#     gae_lambda=0.98,\n",
    "#     gamma=0.999,\n",
    "#     n_epochs=8,\n",
    "#     ent_coef=0.01,\n",
    "#     vf_coef=0.5,\n",
    "#     max_grad_norm=0.5,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=f'./runs/ppo_opt_lunar{\"continuous\" if continuous else \"\"}_tensorboard/'\n",
    "# )\n",
    "# model = PPO(\n",
    "#     \"MlpPolicy\",\n",
    "#     train_env,\n",
    "#     learning_rate=6.93e-4,\n",
    "#     n_steps=512,\n",
    "#     batch_size=32,\n",
    "#     gae_lambda=0.98,\n",
    "#     gamma=0.999,\n",
    "#     n_epochs=4,\n",
    "#     ent_coef=3.253e-3,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=f'./runs/ppo_opt_lunar{\"continuous\" if continuous else \"\"}_tensorboard/'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo_noopt',\n",
    "    log_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo_noopt',\n",
    "    eval_freq=int(1e5 // 16),\n",
    "    n_eval_episodes=10,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/ppo_opt_lunar_tensorboard/PPO_15\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.1     |\n",
      "|    ep_rew_mean     | -194     |\n",
      "| time/              |          |\n",
      "|    fps             | 8498     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 91.6        |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3555        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010434347 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.00191     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 173         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00874    |\n",
      "|    value_loss           | 728         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | -93.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3025        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012534592 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 88.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 287         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-684.87 +/- 481.29\n",
      "Episode length: 566.30 +/- 177.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 566         |\n",
      "|    mean_reward          | -685        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012940022 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 109         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 117      |\n",
      "|    ep_rew_mean     | -64.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 2526     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 128         |\n",
      "|    ep_rew_mean          | -30.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2452        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014518529 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 76.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | -15.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2373        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016139649 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.6        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 58.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-185.97 +/- 53.70\n",
      "Episode length: 557.60 +/- 258.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 558        |\n",
      "|    mean_reward          | -186       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 200000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01695479 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.9        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 17.5       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    value_loss           | 58.6       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 310      |\n",
      "|    ep_rew_mean     | -6.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 2033     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 542         |\n",
      "|    ep_rew_mean          | 25.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1837        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011449088 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 78.3        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 48.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 681         |\n",
      "|    ep_rew_mean          | 38          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1731        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011064159 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00731    |\n",
      "|    value_loss           | 27.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-64.04 +/- 16.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -64         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009472861 |\n",
      "|    clip_fraction        | 0.0792      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.985      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    value_loss           | 34.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 46.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 1559     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 210      |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 814         |\n",
      "|    ep_rew_mean          | 58          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1500        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011061855 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.8         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00502    |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 897         |\n",
      "|    ep_rew_mean          | 85.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1453        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006727054 |\n",
      "|    clip_fraction        | 0.0785      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    value_loss           | 19.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=137.94 +/- 49.91\n",
      "Episode length: 830.50 +/- 61.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 830         |\n",
      "|    mean_reward          | 138         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009656669 |\n",
      "|    clip_fraction        | 0.0888      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.902      |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00386    |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    fps             | 1377     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 309      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 859          |\n",
      "|    ep_rew_mean          | 128          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1365         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 336          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068172878 |\n",
      "|    clip_fraction        | 0.0885       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.817       |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.63         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00386     |\n",
      "|    value_loss           | 24.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 685         |\n",
      "|    ep_rew_mean          | 160         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1365        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011744232 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.763      |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00558    |\n",
      "|    value_loss           | 66.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=210.12 +/- 25.63\n",
      "Episode length: 579.10 +/- 33.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 579         |\n",
      "|    mean_reward          | 210         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015211482 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.774      |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 66.3        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 513      |\n",
      "|    ep_rew_mean     | 184      |\n",
      "| time/              |          |\n",
      "|    fps             | 1352     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 387      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 428         |\n",
      "|    ep_rew_mean          | 192         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1362        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 408         |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008782014 |\n",
      "|    clip_fraction        | 0.08        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.777      |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 171         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 425         |\n",
      "|    ep_rew_mean          | 210         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1373        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 429         |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004829705 |\n",
      "|    clip_fraction        | 0.0411      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.738      |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 97          |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=230.44 +/- 16.74\n",
      "Episode length: 451.10 +/- 24.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 451         |\n",
      "|    mean_reward          | 230         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008499907 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.818      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62.3        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00208    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 360      |\n",
      "|    ep_rew_mean     | 227      |\n",
      "| time/              |          |\n",
      "|    fps             | 1377     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 451      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 326          |\n",
      "|    ep_rew_mean          | 217          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1391         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 470          |\n",
      "|    total_timesteps      | 655360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060835886 |\n",
      "|    clip_fraction        | 0.0708       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.79        |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.18         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    value_loss           | 87.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 369         |\n",
      "|    ep_rew_mean          | 208         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1403        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 490         |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005857677 |\n",
      "|    clip_fraction        | 0.0665      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.767      |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.3        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=244.51 +/- 17.99\n",
      "Episode length: 331.00 +/- 17.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 331          |\n",
      "|    mean_reward          | 245          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 700000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042869244 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.736       |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.5         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 308      |\n",
      "|    ep_rew_mean     | 234      |\n",
      "| time/              |          |\n",
      "|    fps             | 1410     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 510      |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 323          |\n",
      "|    ep_rew_mean          | 221          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1422         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 529          |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036776569 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.71        |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.6         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 85           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 322         |\n",
      "|    ep_rew_mean          | 225         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1434        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003970435 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.2        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 99.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=228.18 +/- 72.96\n",
      "Episode length: 292.90 +/- 16.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 293         |\n",
      "|    mean_reward          | 228         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004743333 |\n",
      "|    clip_fraction        | 0.0498      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.705      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.9        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 92.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 373      |\n",
      "|    ep_rew_mean     | 218      |\n",
      "| time/              |          |\n",
      "|    fps             | 1439     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 569      |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 356          |\n",
      "|    ep_rew_mean          | 227          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1446         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 588          |\n",
      "|    total_timesteps      | 851968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048121884 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.694       |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 80           |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 73.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 335          |\n",
      "|    ep_rew_mean          | 238          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1455         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 607          |\n",
      "|    total_timesteps      | 884736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062067364 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.67        |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.13         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    value_loss           | 66.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=274.17 +/- 15.18\n",
      "Episode length: 320.50 +/- 107.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | 274          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 900000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047937604 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.676       |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 52.2         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 310      |\n",
      "|    ep_rew_mean     | 239      |\n",
      "| time/              |          |\n",
      "|    fps             | 1461     |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 627      |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 353          |\n",
      "|    ep_rew_mean          | 242          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1466         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 648          |\n",
      "|    total_timesteps      | 950272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065500643 |\n",
      "|    clip_fraction        | 0.0767       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.681       |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.98         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 57.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 335          |\n",
      "|    ep_rew_mean          | 253          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1469         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 669          |\n",
      "|    total_timesteps      | 983040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051134927 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.65        |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.2          |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 39.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=259.25 +/- 9.87\n",
      "Episode length: 268.30 +/- 21.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 268         |\n",
      "|    mean_reward          | 259         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006435982 |\n",
      "|    clip_fraction        | 0.0746      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.5        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    value_loss           | 32.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 346      |\n",
      "|    ep_rew_mean     | 248      |\n",
      "| time/              |          |\n",
      "|    fps             | 1466     |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 692      |\n",
      "|    total_timesteps | 1015808  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2614872ba30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e6), callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(f'data/policies/LunarLander{\"Continuous\" if continuous else \"\"}-v2#ppo_2#training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a094b89469df4117159496550a75e1ef6dfce4ed77bce29eb93efb1f0342af31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
