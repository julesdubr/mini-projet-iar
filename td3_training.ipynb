{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "eval_env = gym.make(\"LunarLanderContinuous-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = TD3(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    gamma=0.98,\n",
    "    buffer_size=200000,\n",
    "    learning_starts=10000,\n",
    "    gradient_steps=-1,\n",
    "    train_freq=(1, \"episode\"),\n",
    "    learning_rate=1e-3,\n",
    "    policy_kwargs=dict(net_arch=[400, 300]),\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./runs/td3_lunarcontinuous_tensorboard/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/continuous/td3\",\n",
    "    log_path=\"./logs/continuous/td3\",\n",
    "    eval_freq=1000,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/td3_lunar_tensorboard/TD3_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 80       |\n",
      "|    ep_rew_mean     | -128     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 3377     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 320      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jules Dubreuil\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-298.10 +/- 125.47\n",
      "Episode length: 172.40 +/- 92.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 172      |\n",
      "|    mean_reward     | -298     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.1     |\n",
      "|    ep_rew_mean     | -167     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 559      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 705      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-237.77 +/- 128.96\n",
      "Episode length: 103.20 +/- 9.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 103      |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.2     |\n",
      "|    ep_rew_mean     | -194     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 561      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1083     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.8     |\n",
      "|    ep_rew_mean     | -220     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 721      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 1485     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-287.37 +/- 114.82\n",
      "Episode length: 129.80 +/- 19.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 130      |\n",
      "|    mean_reward     | -287     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97       |\n",
      "|    ep_rew_mean     | -215     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 712      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 1939     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-268.92 +/- 147.93\n",
      "Episode length: 128.00 +/- 41.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 128      |\n",
      "|    mean_reward     | -269     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.5     |\n",
      "|    ep_rew_mean     | -221     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2317     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-323.24 +/- 71.41\n",
      "Episode length: 194.20 +/- 95.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 194      |\n",
      "|    mean_reward     | -323     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.9     |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 606      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2798     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-372.03 +/- 22.87\n",
      "Episode length: 115.60 +/- 20.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 116      |\n",
      "|    mean_reward     | -372     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 612      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 3201     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-194.51 +/- 138.44\n",
      "Episode length: 139.00 +/- 41.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 139      |\n",
      "|    mean_reward     | -195     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 590      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 3619     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-254.73 +/- 135.43\n",
      "Episode length: 129.80 +/- 40.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 130      |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -218     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 573      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 4001     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 4499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-348.20 +/- 62.78\n",
      "Episode length: 116.80 +/- 21.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 117      |\n",
      "|    mean_reward     | -348     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-320.82 +/- 28.84\n",
      "Episode length: 100.60 +/- 25.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 101      |\n",
      "|    mean_reward     | -321     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 588      |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 5004     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 5463     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-292.06 +/- 164.96\n",
      "Episode length: 143.00 +/- 37.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 143      |\n",
      "|    mean_reward     | -292     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 5909     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-244.63 +/- 127.27\n",
      "Episode length: 103.40 +/- 19.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 103      |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -219     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 6368     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-230.35 +/- 67.47\n",
      "Episode length: 125.40 +/- 19.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 125      |\n",
      "|    mean_reward     | -230     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 6804     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-242.54 +/- 139.42\n",
      "Episode length: 157.60 +/- 53.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 158      |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -222     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 7186     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-315.39 +/- 48.42\n",
      "Episode length: 106.60 +/- 17.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 107      |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | -228     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 7576     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-302.51 +/- 56.44\n",
      "Episode length: 168.80 +/- 73.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 169      |\n",
      "|    mean_reward     | -303     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -229     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 603      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 8038     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 627      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 8424     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-316.58 +/- 93.83\n",
      "Episode length: 132.60 +/- 21.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 133      |\n",
      "|    mean_reward     | -317     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 619      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 8782     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-331.64 +/- 91.38\n",
      "Episode length: 113.00 +/- 15.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 113      |\n",
      "|    mean_reward     | -332     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 9290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-224.10 +/- 143.92\n",
      "Episode length: 157.00 +/- 27.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 157      |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | -220     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 608      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 9676     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-338.39 +/- 41.33\n",
      "Episode length: 120.00 +/- 27.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 120      |\n",
      "|    mean_reward     | -338     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 564      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 10207    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.99     |\n",
      "|    critic_loss     | 187      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 139      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-332.57 +/- 82.07\n",
      "Episode length: 149.20 +/- 68.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 149      |\n",
      "|    mean_reward     | -333     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.19     |\n",
      "|    critic_loss     | 129      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 483      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | -232     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 454      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 10725    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.69     |\n",
      "|    critic_loss     | 137      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 640      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-456.62 +/- 187.30\n",
      "Episode length: 151.80 +/- 130.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 152      |\n",
      "|    mean_reward     | -457     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.59     |\n",
      "|    critic_loss     | 97.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 769      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-208.48 +/- 166.59\n",
      "Episode length: 237.60 +/- 231.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 238      |\n",
      "|    mean_reward     | -208     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.47     |\n",
      "|    critic_loss     | 92.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1065     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-212.37 +/- 17.82\n",
      "Episode length: 636.20 +/- 73.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 636      |\n",
      "|    mean_reward     | -212     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.35     |\n",
      "|    critic_loss     | 79.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1913     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=-239.53 +/- 25.80\n",
      "Episode length: 704.40 +/- 101.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 704      |\n",
      "|    mean_reward     | -240     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | -237     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 234      |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 12511    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-274.12 +/- 20.47\n",
      "Episode length: 241.20 +/- 29.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 241      |\n",
      "|    mean_reward     | -274     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.866    |\n",
      "|    critic_loss     | 63       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3032     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=-245.51 +/- 31.93\n",
      "Episode length: 305.00 +/- 86.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 305      |\n",
      "|    mean_reward     | -246     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.309    |\n",
      "|    critic_loss     | 59.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-195.78 +/- 35.28\n",
      "Episode length: 447.80 +/- 114.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 448      |\n",
      "|    mean_reward     | -196     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0611   |\n",
      "|    critic_loss     | 61.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3714     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 134      |\n",
      "|    ep_rew_mean     | -237     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 14075    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=-122.68 +/- 38.26\n",
      "Episode length: 428.40 +/- 54.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 428      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.393   |\n",
      "|    critic_loss     | 51       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4119     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-189.51 +/- 26.99\n",
      "Episode length: 391.60 +/- 99.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 392      |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 51.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=-117.15 +/- 21.50\n",
      "Episode length: 403.60 +/- 152.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 404      |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.79    |\n",
      "|    critic_loss     | 45.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5530     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 147      |\n",
      "|    ep_rew_mean     | -234     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 15735    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-126.36 +/- 15.67\n",
      "Episode length: 328.00 +/- 32.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 328      |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.08    |\n",
      "|    critic_loss     | 50.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5779     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=-141.77 +/- 26.66\n",
      "Episode length: 365.40 +/- 38.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 365      |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.19    |\n",
      "|    critic_loss     | 49.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6248     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-183.13 +/- 17.84\n",
      "Episode length: 702.60 +/- 77.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 703      |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.86    |\n",
      "|    critic_loss     | 54.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6601     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-175.63 +/- 31.83\n",
      "Episode length: 395.40 +/- 156.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 395      |\n",
      "|    mean_reward     | -176     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.94    |\n",
      "|    critic_loss     | 48       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7347     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 162      |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 134      |\n",
      "|    total_timesteps | 17653    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-140.45 +/- 42.61\n",
      "Episode length: 467.80 +/- 129.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 468      |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.66    |\n",
      "|    critic_loss     | 45.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7697     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-134.02 +/- 22.77\n",
      "Episode length: 303.60 +/- 29.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 304      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.19    |\n",
      "|    critic_loss     | 47.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8516     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-114.46 +/- 68.64\n",
      "Episode length: 416.40 +/- 115.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 416      |\n",
      "|    mean_reward     | -114     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.86    |\n",
      "|    critic_loss     | 47.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8757     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 172      |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 19099    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=-108.90 +/- 54.26\n",
      "Episode length: 242.80 +/- 31.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 243      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.98    |\n",
      "|    critic_loss     | 46.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-159.86 +/- 42.67\n",
      "Episode length: 378.40 +/- 106.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 378      |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.81    |\n",
      "|    critic_loss     | 42.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9945     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 179      |\n",
      "|    ep_rew_mean     | -222     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 116      |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 20258    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-102.21 +/- 84.08\n",
      "Episode length: 423.20 +/- 205.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 423      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.58    |\n",
      "|    critic_loss     | 41.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10529    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=-50.85 +/- 150.82\n",
      "Episode length: 390.80 +/- 141.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 391      |\n",
      "|    mean_reward     | -50.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.35    |\n",
      "|    critic_loss     | 41.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10868    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21500, episode_reward=-83.29 +/- 104.63\n",
      "Episode length: 357.60 +/- 124.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 358      |\n",
      "|    mean_reward     | -83.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.36    |\n",
      "|    critic_loss     | 44.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11267    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 187      |\n",
      "|    ep_rew_mean     | -218     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 109      |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 21510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-140.99 +/- 71.26\n",
      "Episode length: 388.60 +/- 228.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 389      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.87    |\n",
      "|    critic_loss     | 42.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11738    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=8.53 +/- 157.66\n",
      "Episode length: 403.60 +/- 98.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 404      |\n",
      "|    mean_reward     | 8.53     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.81    |\n",
      "|    critic_loss     | 45       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12441    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=-47.90 +/- 22.34\n",
      "Episode length: 355.80 +/- 89.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 356      |\n",
      "|    mean_reward     | -47.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.25    |\n",
      "|    critic_loss     | 48.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12715    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=-88.65 +/- 41.32\n",
      "Episode length: 293.80 +/- 97.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 294      |\n",
      "|    mean_reward     | -88.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 203      |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 105      |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 23502    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=58.90 +/- 129.55\n",
      "Episode length: 387.20 +/- 167.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 387      |\n",
      "|    mean_reward     | 58.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.45    |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13746    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24500, episode_reward=-33.00 +/- 30.19\n",
      "Episode length: 198.60 +/- 24.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 199      |\n",
      "|    mean_reward     | -33      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.89    |\n",
      "|    critic_loss     | 37.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14334    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 213      |\n",
      "|    ep_rew_mean     | -196     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 248      |\n",
      "|    total_timesteps | 24898    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.56    |\n",
      "|    critic_loss     | 37.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14703    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=28.65 +/- 96.30\n",
      "Episode length: 261.60 +/- 72.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 262      |\n",
      "|    mean_reward     | 28.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.23    |\n",
      "|    critic_loss     | 33.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14942    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=-12.43 +/- 143.01\n",
      "Episode length: 289.40 +/- 92.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 289      |\n",
      "|    mean_reward     | -12.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.31    |\n",
      "|    critic_loss     | 40.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15403    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-54.14 +/- 26.41\n",
      "Episode length: 240.40 +/- 35.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 240      |\n",
      "|    mean_reward     | -54.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.16    |\n",
      "|    critic_loss     | 40.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15818    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 26015    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=46.28 +/- 80.18\n",
      "Episode length: 274.80 +/- 51.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 275      |\n",
      "|    mean_reward     | 46.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.42    |\n",
      "|    critic_loss     | 39.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16381    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=52.22 +/- 119.51\n",
      "Episode length: 537.80 +/- 288.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 538      |\n",
      "|    mean_reward     | 52.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.63    |\n",
      "|    critic_loss     | 34.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16897    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 227      |\n",
      "|    ep_rew_mean     | -177     |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 27162    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=116.59 +/- 140.34\n",
      "Episode length: 390.00 +/- 98.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 390      |\n",
      "|    mean_reward     | 117      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.27    |\n",
      "|    critic_loss     | 31.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17206    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=-34.83 +/- 43.25\n",
      "Episode length: 272.00 +/- 100.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 272      |\n",
      "|    mean_reward     | -34.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.11    |\n",
      "|    critic_loss     | 34       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17963    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=110.99 +/- 108.77\n",
      "Episode length: 310.60 +/- 54.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 311      |\n",
      "|    mean_reward     | 111      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.37    |\n",
      "|    critic_loss     | 30.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18522    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 237      |\n",
      "|    ep_rew_mean     | -166     |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 303      |\n",
      "|    total_timesteps | 28686    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=12.99 +/- 36.08\n",
      "Episode length: 202.80 +/- 41.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 203      |\n",
      "|    mean_reward     | 13       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.69    |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19025    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=41.72 +/- 76.69\n",
      "Episode length: 287.00 +/- 112.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 287      |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.54    |\n",
      "|    critic_loss     | 32.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19200    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 243      |\n",
      "|    ep_rew_mean     | -150     |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 318      |\n",
      "|    total_timesteps | 29803    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.43    |\n",
      "|    critic_loss     | 33.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19676    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-13.53 +/- 25.06\n",
      "Episode length: 272.20 +/- 111.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 272      |\n",
      "|    mean_reward     | -13.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.46    |\n",
      "|    critic_loss     | 38.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20033    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=-54.83 +/- 245.62\n",
      "Episode length: 373.40 +/- 128.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 373      |\n",
      "|    mean_reward     | -54.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.02    |\n",
      "|    critic_loss     | 29.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20462    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 247      |\n",
      "|    ep_rew_mean     | -141     |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 30584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=39.24 +/- 77.01\n",
      "Episode length: 261.00 +/- 45.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 261      |\n",
      "|    mean_reward     | 39.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.15    |\n",
      "|    critic_loss     | 32.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=177.17 +/- 86.44\n",
      "Episode length: 358.20 +/- 115.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 358      |\n",
      "|    mean_reward     | 177      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.66    |\n",
      "|    critic_loss     | 33.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21410    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 253      |\n",
      "|    ep_rew_mean     | -135     |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 343      |\n",
      "|    total_timesteps | 31638    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=116.24 +/- 105.48\n",
      "Episode length: 343.00 +/- 96.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 343      |\n",
      "|    mean_reward     | 116      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.54    |\n",
      "|    critic_loss     | 31.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21935    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=73.19 +/- 182.51\n",
      "Episode length: 411.00 +/- 135.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 411      |\n",
      "|    mean_reward     | 73.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.71    |\n",
      "|    critic_loss     | 34       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 22294    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=153.79 +/- 78.49\n",
      "Episode length: 425.20 +/- 108.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 425      |\n",
      "|    mean_reward     | 154      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.87    |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 22629    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 263      |\n",
      "|    ep_rew_mean     | -115     |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 365      |\n",
      "|    total_timesteps | 33056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-98.95 +/- 188.65\n",
      "Episode length: 333.80 +/- 87.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 334      |\n",
      "|    mean_reward     | -98.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.41    |\n",
      "|    critic_loss     | 48.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23355    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-3.67 +/- 134.91\n",
      "Episode length: 283.60 +/- 106.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -3.67    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.6     |\n",
      "|    critic_loss     | 31.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 270      |\n",
      "|    ep_rew_mean     | -105     |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 381      |\n",
      "|    total_timesteps | 34153    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=109.17 +/- 91.87\n",
      "Episode length: 287.00 +/- 97.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 287      |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.5     |\n",
      "|    critic_loss     | 42.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 24421    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=81.35 +/- 173.35\n",
      "Episode length: 486.40 +/- 100.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 486      |\n",
      "|    mean_reward     | 81.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.01    |\n",
      "|    critic_loss     | 40.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 24970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=-51.54 +/- 225.12\n",
      "Episode length: 687.80 +/- 260.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 688      |\n",
      "|    mean_reward     | -51.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | -86      |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 87       |\n",
      "|    time_elapsed    | 406      |\n",
      "|    total_timesteps | 35508    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=37.18 +/- 60.83\n",
      "Episode length: 941.00 +/- 118.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 941      |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.67    |\n",
      "|    critic_loss     | 39.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 25812    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=47.77 +/- 174.17\n",
      "Episode length: 832.20 +/- 259.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 832      |\n",
      "|    mean_reward     | 47.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=124.25 +/- 80.58\n",
      "Episode length: 557.80 +/- 236.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 558      |\n",
      "|    mean_reward     | 124      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.65    |\n",
      "|    critic_loss     | 35.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 26812    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=177.41 +/- 93.96\n",
      "Episode length: 357.00 +/- 61.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 357      |\n",
      "|    mean_reward     | 177      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.72    |\n",
      "|    critic_loss     | 36       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 27288    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 296      |\n",
      "|    ep_rew_mean     | -72      |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 458      |\n",
      "|    total_timesteps | 37648    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=138.31 +/- 198.80\n",
      "Episode length: 535.20 +/- 211.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 535      |\n",
      "|    mean_reward     | 138      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.58    |\n",
      "|    critic_loss     | 35.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 28020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=181.07 +/- 79.44\n",
      "Episode length: 360.20 +/- 83.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 360      |\n",
      "|    mean_reward     | 181      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.61    |\n",
      "|    critic_loss     | 38.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 28501    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=166.42 +/- 50.66\n",
      "Episode length: 721.60 +/- 214.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 722      |\n",
      "|    mean_reward     | 166      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.83    |\n",
      "|    critic_loss     | 34.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 28928    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 310      |\n",
      "|    ep_rew_mean     | -64      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 80       |\n",
      "|    time_elapsed    | 492      |\n",
      "|    total_timesteps | 39457    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=92.32 +/- 95.11\n",
      "Episode length: 806.40 +/- 259.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 92.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.69    |\n",
      "|    critic_loss     | 38.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 29501    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=96.88 +/- 79.77\n",
      "Episode length: 687.00 +/- 310.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 687      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=88.25 +/- 67.04\n",
      "Episode length: 920.60 +/- 98.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 921      |\n",
      "|    mean_reward     | 88.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.62    |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 30501    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=114.75 +/- 94.05\n",
      "Episode length: 728.40 +/- 228.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 728      |\n",
      "|    mean_reward     | 115      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.59    |\n",
      "|    critic_loss     | 35.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 30941    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=124.78 +/- 73.35\n",
      "Episode length: 738.00 +/- 230.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 738      |\n",
      "|    mean_reward     | 125      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.7     |\n",
      "|    critic_loss     | 33.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 31480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 331      |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 74       |\n",
      "|    time_elapsed    | 564      |\n",
      "|    total_timesteps | 41925    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=104.14 +/- 130.64\n",
      "Episode length: 482.40 +/- 281.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 482      |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.67    |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 31969    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=137.45 +/- 48.76\n",
      "Episode length: 830.60 +/- 228.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 831      |\n",
      "|    mean_reward     | 137      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.07    |\n",
      "|    critic_loss     | 34.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 32478    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-184.05 +/- 621.90\n",
      "Episode length: 745.80 +/- 207.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 746      |\n",
      "|    mean_reward     | -184     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=151.38 +/- 48.22\n",
      "Episode length: 858.40 +/- 163.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 858      |\n",
      "|    mean_reward     | 151      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.22    |\n",
      "|    critic_loss     | 36.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 33504    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=124.98 +/- 68.17\n",
      "Episode length: 861.40 +/- 179.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 861      |\n",
      "|    mean_reward     | 125      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 352      |\n",
      "|    ep_rew_mean     | -31.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 626      |\n",
      "|    total_timesteps | 44460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=199.45 +/- 49.99\n",
      "Episode length: 544.00 +/- 193.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 544      |\n",
      "|    mean_reward     | 199      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.67    |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 34504    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=141.14 +/- 112.96\n",
      "Episode length: 323.40 +/- 75.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 323      |\n",
      "|    mean_reward     | 141      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.38    |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 34800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=225.23 +/- 47.78\n",
      "Episode length: 419.80 +/- 141.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 420      |\n",
      "|    mean_reward     | 225      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.14    |\n",
      "|    critic_loss     | 38.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 35216    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=46000, episode_reward=-65.91 +/- 239.55\n",
      "Episode length: 628.60 +/- 332.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 629      |\n",
      "|    mean_reward     | -65.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.28    |\n",
      "|    critic_loss     | 38.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 35739    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 367      |\n",
      "|    ep_rew_mean     | -58      |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 669      |\n",
      "|    total_timesteps | 46407    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=204.82 +/- 31.93\n",
      "Episode length: 535.40 +/- 119.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 535      |\n",
      "|    mean_reward     | 205      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.33    |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 36451    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=198.56 +/- 41.99\n",
      "Episode length: 464.60 +/- 153.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 465      |\n",
      "|    mean_reward     | 199      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.71    |\n",
      "|    critic_loss     | 55.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 36870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=216.08 +/- 26.70\n",
      "Episode length: 472.00 +/- 163.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 472      |\n",
      "|    mean_reward     | 216      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=101.06 +/- 98.16\n",
      "Episode length: 538.20 +/- 305.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 538      |\n",
      "|    mean_reward     | 101      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.71    |\n",
      "|    critic_loss     | 34.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 37954    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 378      |\n",
      "|    ep_rew_mean     | -36.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 711      |\n",
      "|    total_timesteps | 48049    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=37.70 +/- 79.26\n",
      "Episode length: 614.40 +/- 336.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 614      |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.57    |\n",
      "|    critic_loss     | 43.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 38379    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=139.17 +/- 77.21\n",
      "Episode length: 438.60 +/- 182.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 439      |\n",
      "|    mean_reward     | 139      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.05    |\n",
      "|    critic_loss     | 43.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 38781    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=-148.07 +/- 175.27\n",
      "Episode length: 742.80 +/- 247.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 743      |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.56    |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 39139    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 391      |\n",
      "|    ep_rew_mean     | -14.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 66       |\n",
      "|    time_elapsed    | 744      |\n",
      "|    total_timesteps | 49801    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=10.27 +/- 53.37\n",
      "Episode length: 814.60 +/- 341.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 815      |\n",
      "|    mean_reward     | 10.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.68    |\n",
      "|    critic_loss     | 42.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 39845    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=-70.69 +/- 104.22\n",
      "Episode length: 987.80 +/- 16.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 988      |\n",
      "|    mean_reward     | -70.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-168.86 +/- 58.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -169     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.84    |\n",
      "|    critic_loss     | 34       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 40845    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=-162.35 +/- 15.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=93.80 +/- 161.79\n",
      "Episode length: 590.60 +/- 219.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 591      |\n",
      "|    mean_reward     | 93.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.79    |\n",
      "|    critic_loss     | 38.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 41845    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=8.04 +/- 152.39\n",
      "Episode length: 762.00 +/- 224.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 762      |\n",
      "|    mean_reward     | 8.04     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-153.58 +/- 207.69\n",
      "Episode length: 630.00 +/- 111.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 630      |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.88    |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 42712    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=-111.24 +/- 198.25\n",
      "Episode length: 507.00 +/- 149.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 507      |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 412      |\n",
      "|    ep_rew_mean     | -4.47    |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 880      |\n",
      "|    total_timesteps | 53668    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=79.60 +/- 152.01\n",
      "Episode length: 567.40 +/- 233.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 567      |\n",
      "|    mean_reward     | 79.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.38    |\n",
      "|    critic_loss     | 48.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 43862    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=-32.55 +/- 200.47\n",
      "Episode length: 571.20 +/- 306.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 571      |\n",
      "|    mean_reward     | -32.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.26    |\n",
      "|    critic_loss     | 40.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 44262    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 408      |\n",
      "|    ep_rew_mean     | 5.18     |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 915      |\n",
      "|    total_timesteps | 54921    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.19    |\n",
      "|    critic_loss     | 36.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 44682    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-294.33 +/- 67.08\n",
      "Episode length: 651.20 +/- 251.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 651      |\n",
      "|    mean_reward     | -294     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8       |\n",
      "|    critic_loss     | 34.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 44965    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=-267.19 +/- 144.35\n",
      "Episode length: 548.60 +/- 225.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 549      |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-117.64 +/- 21.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.79    |\n",
      "|    critic_loss     | 50.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45561    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=-139.69 +/- 37.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=175.46 +/- 14.19\n",
      "Episode length: 571.20 +/- 60.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 571      |\n",
      "|    mean_reward     | 175      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.87    |\n",
      "|    critic_loss     | 36.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 46561    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=42.62 +/- 86.84\n",
      "Episode length: 888.00 +/- 102.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 888      |\n",
      "|    mean_reward     | 42.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.63    |\n",
      "|    critic_loss     | 28.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 47070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=53.14 +/- 101.92\n",
      "Episode length: 635.80 +/- 330.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 636      |\n",
      "|    mean_reward     | 53.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 423      |\n",
      "|    ep_rew_mean     | 9.95     |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 58       |\n",
      "|    time_elapsed    | 996      |\n",
      "|    total_timesteps | 58026    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=-94.04 +/- 13.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -94      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.22    |\n",
      "|    critic_loss     | 37.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 48070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-86.37 +/- 18.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=9.12 +/- 233.47\n",
      "Episode length: 733.60 +/- 197.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 734      |\n",
      "|    mean_reward     | 9.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.66    |\n",
      "|    critic_loss     | 29.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 49070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-2.17 +/- 94.80\n",
      "Episode length: 952.20 +/- 70.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 952      |\n",
      "|    mean_reward     | -2.17    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.23    |\n",
      "|    critic_loss     | 34.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 49614    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=-59.20 +/- 29.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -59.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.22    |\n",
      "|    critic_loss     | 43.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 50267    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-47.72 +/- 46.63\n",
      "Episode length: 829.80 +/- 340.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 830      |\n",
      "|    mean_reward     | -47.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 436      |\n",
      "|    ep_rew_mean     | 16       |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 1098     |\n",
      "|    total_timesteps | 61223    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=73.48 +/- 120.04\n",
      "Episode length: 722.40 +/- 339.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 722      |\n",
      "|    mean_reward     | 73.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.24    |\n",
      "|    critic_loss     | 36.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 51267    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-40.35 +/- 124.92\n",
      "Episode length: 879.40 +/- 241.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 879      |\n",
      "|    mean_reward     | -40.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=-108.56 +/- 14.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.37    |\n",
      "|    critic_loss     | 34.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 52267    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-16.64 +/- 127.97\n",
      "Episode length: 853.40 +/- 293.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | -16.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=-110.10 +/- 27.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.27    |\n",
      "|    critic_loss     | 33.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 53267    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-101.75 +/- 32.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=-109.62 +/- 14.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.45    |\n",
      "|    critic_loss     | 32       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 54267    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-104.68 +/- 26.06\n",
      "Episode length: 904.40 +/- 191.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 904      |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 461      |\n",
      "|    ep_rew_mean     | 19.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 52       |\n",
      "|    time_elapsed    | 1243     |\n",
      "|    total_timesteps | 65223    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=-172.96 +/- 49.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -173     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.64    |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 55267    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-63.09 +/- 146.79\n",
      "Episode length: 876.80 +/- 246.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 877      |\n",
      "|    mean_reward     | -63.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=-77.18 +/- 60.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.49    |\n",
      "|    critic_loss     | 27.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 56267    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-11.28 +/- 100.02\n",
      "Episode length: 733.60 +/- 326.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 734      |\n",
      "|    mean_reward     | -11.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=79.42 +/- 106.04\n",
      "Episode length: 960.00 +/- 48.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 960      |\n",
      "|    mean_reward     | 79.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.63    |\n",
      "|    critic_loss     | 29.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 57267    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=25.36 +/- 123.75\n",
      "Episode length: 784.20 +/- 178.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 784      |\n",
      "|    mean_reward     | 25.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=191.30 +/- 49.31\n",
      "Episode length: 708.40 +/- 153.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 708      |\n",
      "|    mean_reward     | 191      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.44    |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 58222    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=139.79 +/- 83.83\n",
      "Episode length: 775.00 +/- 186.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 775      |\n",
      "|    mean_reward     | 140      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 488      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 49       |\n",
      "|    time_elapsed    | 1387     |\n",
      "|    total_timesteps | 69077    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=85.71 +/- 136.46\n",
      "Episode length: 788.60 +/- 130.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 789      |\n",
      "|    mean_reward     | 85.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.46    |\n",
      "|    critic_loss     | 23.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 59517    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=108.15 +/- 74.71\n",
      "Episode length: 913.60 +/- 82.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 914      |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=-67.77 +/- 27.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -67.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.22    |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 60443    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-120.83 +/- 242.67\n",
      "Episode length: 825.20 +/- 297.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 825      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=92.30 +/- 114.89\n",
      "Episode length: 599.20 +/- 208.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 599      |\n",
      "|    mean_reward     | 92.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.03    |\n",
      "|    critic_loss     | 28.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 61443    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=92.48 +/- 80.82\n",
      "Episode length: 588.20 +/- 161.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 588      |\n",
      "|    mean_reward     | 92.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 506      |\n",
      "|    ep_rew_mean     | 33.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 1488     |\n",
      "|    total_timesteps | 72084    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=154.96 +/- 79.56\n",
      "Episode length: 670.40 +/- 250.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 670      |\n",
      "|    mean_reward     | 155      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.97    |\n",
      "|    critic_loss     | 25.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 62128    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=115.89 +/- 120.25\n",
      "Episode length: 637.00 +/- 154.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 637      |\n",
      "|    mean_reward     | 116      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.28    |\n",
      "|    critic_loss     | 26.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 62909    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=142.02 +/- 149.12\n",
      "Episode length: 519.20 +/- 181.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 519      |\n",
      "|    mean_reward     | 142      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.39    |\n",
      "|    critic_loss     | 28       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 63363    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=71.71 +/- 133.41\n",
      "Episode length: 547.60 +/- 138.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 548      |\n",
      "|    mean_reward     | 71.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 507      |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 1533     |\n",
      "|    total_timesteps | 74193    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=15.92 +/- 166.58\n",
      "Episode length: 603.80 +/- 238.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 604      |\n",
      "|    mean_reward     | 15.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.52    |\n",
      "|    critic_loss     | 30.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 64237    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-62.43 +/- 137.20\n",
      "Episode length: 622.40 +/- 131.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 622      |\n",
      "|    mean_reward     | -62.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.35    |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 64830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=110.83 +/- 95.90\n",
      "Episode length: 887.00 +/- 71.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 887      |\n",
      "|    mean_reward     | 111      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.43    |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 65293    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=28.94 +/- 125.52\n",
      "Episode length: 949.60 +/- 98.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 950      |\n",
      "|    mean_reward     | 28.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.27    |\n",
      "|    critic_loss     | 26.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 65740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=2.85 +/- 141.10\n",
      "Episode length: 886.80 +/- 124.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 887      |\n",
      "|    mean_reward     | 2.85     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 518      |\n",
      "|    ep_rew_mean     | 37       |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 1606     |\n",
      "|    total_timesteps | 76696    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-78.89 +/- 82.55\n",
      "Episode length: 774.20 +/- 277.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 774      |\n",
      "|    mean_reward     | -78.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.3     |\n",
      "|    critic_loss     | 31.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 66740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=-16.18 +/- 136.73\n",
      "Episode length: 780.40 +/- 195.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 780      |\n",
      "|    mean_reward     | -16.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.38    |\n",
      "|    critic_loss     | 25.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 67437    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=88.23 +/- 77.71\n",
      "Episode length: 828.80 +/- 253.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 829      |\n",
      "|    mean_reward     | 88.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=-35.58 +/- 134.43\n",
      "Episode length: 738.80 +/- 322.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 739      |\n",
      "|    mean_reward     | -35.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.44    |\n",
      "|    critic_loss     | 24.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 68437    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=23.70 +/- 154.92\n",
      "Episode length: 740.40 +/- 317.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 740      |\n",
      "|    mean_reward     | 23.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=-92.79 +/- 10.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.66    |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 69437    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-32.34 +/- 127.11\n",
      "Episode length: 849.60 +/- 300.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 850      |\n",
      "|    mean_reward     | -32.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 544      |\n",
      "|    ep_rew_mean     | 32.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 1704     |\n",
      "|    total_timesteps | 80393    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=-34.83 +/- 119.92\n",
      "Episode length: 855.80 +/- 288.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 856      |\n",
      "|    mean_reward     | -34.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.59    |\n",
      "|    critic_loss     | 27.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 70437    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-18.47 +/- 140.32\n",
      "Episode length: 845.60 +/- 308.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 846      |\n",
      "|    mean_reward     | -18.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=92.09 +/- 149.03\n",
      "Episode length: 592.00 +/- 335.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 592      |\n",
      "|    mean_reward     | 92.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.69    |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 71437    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-2.20 +/- 134.84\n",
      "Episode length: 543.40 +/- 373.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 543      |\n",
      "|    mean_reward     | -2.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=77.68 +/- 145.90\n",
      "Episode length: 683.60 +/- 276.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 684      |\n",
      "|    mean_reward     | 77.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.65    |\n",
      "|    critic_loss     | 26.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 72437    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 558      |\n",
      "|    ep_rew_mean     | 28.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 1775     |\n",
      "|    total_timesteps | 82915    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.11    |\n",
      "|    critic_loss     | 27.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 72761    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=78.44 +/- 91.14\n",
      "Episode length: 661.40 +/- 332.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 661      |\n",
      "|    mean_reward     | 78.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.83    |\n",
      "|    critic_loss     | 39.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 72959    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=-15.84 +/- 78.68\n",
      "Episode length: 891.80 +/- 185.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 892      |\n",
      "|    mean_reward     | -15.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=33.72 +/- 160.01\n",
      "Episode length: 638.80 +/- 220.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 639      |\n",
      "|    mean_reward     | 33.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.86    |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 73855    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=-43.67 +/- 177.40\n",
      "Episode length: 559.60 +/- 224.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 560      |\n",
      "|    mean_reward     | -43.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=72.07 +/- 155.48\n",
      "Episode length: 661.40 +/- 289.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 661      |\n",
      "|    mean_reward     | 72.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.03    |\n",
      "|    critic_loss     | 28.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 74755    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=4.87 +/- 107.41\n",
      "Episode length: 771.60 +/- 283.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 772      |\n",
      "|    mean_reward     | 4.87     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-34.19 +/- 143.36\n",
      "Episode length: 597.20 +/- 222.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 597      |\n",
      "|    mean_reward     | -34.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.83    |\n",
      "|    critic_loss     | 26.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 75755    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=38.11 +/- 132.11\n",
      "Episode length: 675.20 +/- 272.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 675      |\n",
      "|    mean_reward     | 38.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 580      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 1868     |\n",
      "|    total_timesteps | 86711    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=85.96 +/- 158.36\n",
      "Episode length: 538.80 +/- 299.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 539      |\n",
      "|    mean_reward     | 86       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.82    |\n",
      "|    critic_loss     | 27.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 76755    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=68.86 +/- 123.89\n",
      "Episode length: 701.00 +/- 366.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 701      |\n",
      "|    mean_reward     | 68.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=52.92 +/- 146.49\n",
      "Episode length: 741.00 +/- 317.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 741      |\n",
      "|    mean_reward     | 52.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.72    |\n",
      "|    critic_loss     | 23.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 77755    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=-59.06 +/- 13.93\n",
      "Episode length: 846.40 +/- 307.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 846      |\n",
      "|    mean_reward     | -59.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-13.14 +/- 123.43\n",
      "Episode length: 865.80 +/- 268.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 866      |\n",
      "|    mean_reward     | -13.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.72    |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 78755    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=93.80 +/- 154.00\n",
      "Episode length: 730.20 +/- 240.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 730      |\n",
      "|    mean_reward     | 93.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.04    |\n",
      "|    critic_loss     | 25.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 79059    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=18.46 +/- 123.96\n",
      "Episode length: 754.00 +/- 301.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 754      |\n",
      "|    mean_reward     | 18.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 1951     |\n",
      "|    total_timesteps | 90015    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=149.14 +/- 103.83\n",
      "Episode length: 714.00 +/- 267.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 714      |\n",
      "|    mean_reward     | 149      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.86    |\n",
      "|    critic_loss     | 26.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 80059    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=68.78 +/- 148.85\n",
      "Episode length: 750.60 +/- 309.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 751      |\n",
      "|    mean_reward     | 68.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=-4.29 +/- 128.02\n",
      "Episode length: 901.00 +/- 198.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 901      |\n",
      "|    mean_reward     | -4.29    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.67    |\n",
      "|    critic_loss     | 27.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 81059    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=186.85 +/- 118.48\n",
      "Episode length: 511.60 +/- 251.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 512      |\n",
      "|    mean_reward     | 187      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=60.24 +/- 154.19\n",
      "Episode length: 759.80 +/- 294.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 760      |\n",
      "|    mean_reward     | 60.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.64    |\n",
      "|    critic_loss     | 23.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 82059    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=51.34 +/- 144.37\n",
      "Episode length: 776.00 +/- 275.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 776      |\n",
      "|    mean_reward     | 51.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.35    |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 82714    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 625      |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 2031     |\n",
      "|    total_timesteps | 93107    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=-62.97 +/- 22.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.32    |\n",
      "|    critic_loss     | 21.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 83529    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-63.59 +/- 10.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.41    |\n",
      "|    critic_loss     | 29       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 84003    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=-61.76 +/- 25.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -61.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=141.20 +/- 231.11\n",
      "Episode length: 386.00 +/- 111.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 386      |\n",
      "|    mean_reward     | 141      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.2     |\n",
      "|    critic_loss     | 30.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 85003    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=87.17 +/- 145.07\n",
      "Episode length: 706.80 +/- 359.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 707      |\n",
      "|    mean_reward     | 87.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 643      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 2100     |\n",
      "|    total_timesteps | 95959    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=73.64 +/- 156.07\n",
      "Episode length: 665.40 +/- 279.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 665      |\n",
      "|    mean_reward     | 73.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.13    |\n",
      "|    critic_loss     | 27.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 86003    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=-22.83 +/- 130.75\n",
      "Episode length: 581.40 +/- 347.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 581      |\n",
      "|    mean_reward     | -22.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-21.59 +/- 127.01\n",
      "Episode length: 889.20 +/- 221.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 889      |\n",
      "|    mean_reward     | -21.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.21    |\n",
      "|    critic_loss     | 28.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 87003    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=122.25 +/- 168.84\n",
      "Episode length: 484.80 +/- 271.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 485      |\n",
      "|    mean_reward     | 122      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.33    |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 87410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=116.87 +/- 151.35\n",
      "Episode length: 674.80 +/- 266.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 675      |\n",
      "|    mean_reward     | 117      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.99    |\n",
      "|    critic_loss     | 26       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 87686    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 650      |\n",
      "|    ep_rew_mean     | 19.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 2157     |\n",
      "|    total_timesteps | 98023    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=-32.01 +/- 86.89\n",
      "Episode length: 828.60 +/- 283.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 829      |\n",
      "|    mean_reward     | -32      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.93    |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 88507    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=98.15 +/- 158.45\n",
      "Episode length: 717.20 +/- 205.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 717      |\n",
      "|    mean_reward     | 98.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=64.36 +/- 143.04\n",
      "Episode length: 302.80 +/- 88.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 303      |\n",
      "|    mean_reward     | 64.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.82    |\n",
      "|    critic_loss     | 25.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 89396    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=118.36 +/- 138.45\n",
      "Episode length: 628.80 +/- 324.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 629      |\n",
      "|    mean_reward     | 118      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 662      |\n",
      "|    ep_rew_mean     | 18.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 2204     |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=-1.81 +/- 101.66\n",
      "Episode length: 776.40 +/- 277.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 776      |\n",
      "|    mean_reward     | -1.81    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.82    |\n",
      "|    critic_loss     | 26.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 90396    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=47.53 +/- 124.63\n",
      "Episode length: 835.00 +/- 203.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 835      |\n",
      "|    mean_reward     | 47.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.57    |\n",
      "|    critic_loss     | 23.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 90816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=11.53 +/- 92.60\n",
      "Episode length: 635.20 +/- 311.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 635      |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=26.53 +/- 126.38\n",
      "Episode length: 865.40 +/- 269.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 865      |\n",
      "|    mean_reward     | 26.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.59    |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 91816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102500, episode_reward=108.14 +/- 149.20\n",
      "Episode length: 472.80 +/- 91.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 473      |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=46.55 +/- 136.01\n",
      "Episode length: 778.40 +/- 272.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 778      |\n",
      "|    mean_reward     | 46.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.32    |\n",
      "|    critic_loss     | 26.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 92816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=-92.56 +/- 10.69\n",
      "Episode length: 918.00 +/- 164.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 918      |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 683      |\n",
      "|    ep_rew_mean     | 13.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 2291     |\n",
      "|    total_timesteps | 103772   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-3.46 +/- 88.99\n",
      "Episode length: 946.80 +/- 106.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 947      |\n",
      "|    mean_reward     | -3.46    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.01    |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 93816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=-43.62 +/- 64.12\n",
      "Episode length: 978.40 +/- 43.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 978      |\n",
      "|    mean_reward     | -43.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=155.20 +/- 52.46\n",
      "Episode length: 725.60 +/- 121.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 726      |\n",
      "|    mean_reward     | 155      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.72    |\n",
      "|    critic_loss     | 27.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 94816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=-4.75 +/- 138.08\n",
      "Episode length: 819.40 +/- 184.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 819      |\n",
      "|    mean_reward     | -4.75    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=195.63 +/- 47.81\n",
      "Episode length: 719.20 +/- 240.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 719      |\n",
      "|    mean_reward     | 196      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.72    |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 95608    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=126.85 +/- 112.18\n",
      "Episode length: 804.00 +/- 229.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 127      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=59.09 +/- 160.82\n",
      "Episode length: 811.20 +/- 152.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 811      |\n",
      "|    mean_reward     | 59.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.65    |\n",
      "|    critic_loss     | 26       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 96608    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 694      |\n",
      "|    ep_rew_mean     | 7.73     |\n",
      "| time/              |          |\n",
      "|    episodes        | 276      |\n",
      "|    fps             | 44       |\n",
      "|    time_elapsed    | 2402     |\n",
      "|    total_timesteps | 107068   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107500, episode_reward=-15.81 +/- 111.40\n",
      "Episode length: 771.00 +/- 259.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 771      |\n",
      "|    mean_reward     | -15.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.65    |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 97112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-4.91 +/- 109.85\n",
      "Episode length: 938.00 +/- 124.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 938      |\n",
      "|    mean_reward     | -4.91    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=-127.15 +/- 117.24\n",
      "Episode length: 822.60 +/- 224.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 823      |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.86    |\n",
      "|    critic_loss     | 30.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 98441    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-86.55 +/- 137.09\n",
      "Episode length: 508.20 +/- 252.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 508      |\n",
      "|    mean_reward     | -86.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=-17.44 +/- 121.66\n",
      "Episode length: 781.60 +/- 276.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 782      |\n",
      "|    mean_reward     | -17.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.58    |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 99441    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-124.32 +/- 218.30\n",
      "Episode length: 551.60 +/- 175.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 552      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 708      |\n",
      "|    ep_rew_mean     | 7.22     |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 44       |\n",
      "|    time_elapsed    | 2478     |\n",
      "|    total_timesteps | 110291   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=-91.69 +/- 155.58\n",
      "Episode length: 852.80 +/- 155.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | -91.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.78    |\n",
      "|    critic_loss     | 23.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 100335   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=44.05 +/- 127.31\n",
      "Episode length: 537.60 +/- 167.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 538      |\n",
      "|    mean_reward     | 44       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 111000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.56    |\n",
      "|    critic_loss     | 26.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 101025   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=172.76 +/- 61.94\n",
      "Episode length: 594.20 +/- 264.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 594      |\n",
      "|    mean_reward     | 173      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 111500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-97.28 +/- 13.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.32    |\n",
      "|    critic_loss     | 26.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 102025   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=-56.08 +/- 53.49\n",
      "Episode length: 845.80 +/- 308.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 846      |\n",
      "|    mean_reward     | -56.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-15.69 +/- 132.25\n",
      "Episode length: 868.00 +/- 264.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 868      |\n",
      "|    mean_reward     | -15.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.62    |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 103025   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=-0.91 +/- 134.10\n",
      "Episode length: 882.60 +/- 234.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 883      |\n",
      "|    mean_reward     | -0.912   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 721      |\n",
      "|    ep_rew_mean     | -4.87    |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 44       |\n",
      "|    time_elapsed    | 2582     |\n",
      "|    total_timesteps | 113981   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-38.31 +/- 126.93\n",
      "Episode length: 922.60 +/- 154.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 923      |\n",
      "|    mean_reward     | -38.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.1     |\n",
      "|    critic_loss     | 21.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 104025   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=-11.89 +/- 120.00\n",
      "Episode length: 922.20 +/- 155.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 922      |\n",
      "|    mean_reward     | -11.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=49.36 +/- 113.00\n",
      "Episode length: 926.80 +/- 146.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 927      |\n",
      "|    mean_reward     | 49.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.02    |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 105025   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=65.18 +/- 150.39\n",
      "Episode length: 793.00 +/- 260.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 793      |\n",
      "|    mean_reward     | 65.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-27.85 +/- 47.02\n",
      "Episode length: 847.20 +/- 305.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 847      |\n",
      "|    mean_reward     | -27.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.06    |\n",
      "|    critic_loss     | 21.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 106025   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=-71.65 +/- 11.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -71.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=16.06 +/- 104.44\n",
      "Episode length: 892.60 +/- 214.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 893      |\n",
      "|    mean_reward     | 16.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.21    |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 107025   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=81.73 +/- 141.67\n",
      "Episode length: 801.00 +/- 244.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 801      |\n",
      "|    mean_reward     | 81.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 735      |\n",
      "|    ep_rew_mean     | -15.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 2717     |\n",
      "|    total_timesteps | 117981   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=80.67 +/- 121.38\n",
      "Episode length: 809.00 +/- 237.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 809      |\n",
      "|    mean_reward     | 80.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.19    |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 108025   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118500, episode_reward=103.67 +/- 79.14\n",
      "Episode length: 942.40 +/- 36.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 942      |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.33    |\n",
      "|    critic_loss     | 29.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 108532   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=127.16 +/- 60.76\n",
      "Episode length: 912.20 +/- 65.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 912      |\n",
      "|    mean_reward     | 127      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 119000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=109.01 +/- 119.23\n",
      "Episode length: 792.00 +/- 246.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 792      |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 119500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.18    |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 109295   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=14.26 +/- 66.69\n",
      "Episode length: 957.80 +/- 84.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 958      |\n",
      "|    mean_reward     | 14.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.39    |\n",
      "|    critic_loss     | 22.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 109927   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 739      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 42       |\n",
      "|    time_elapsed    | 2814     |\n",
      "|    total_timesteps | 120353   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=45.28 +/- 102.63\n",
      "Episode length: 767.60 +/- 297.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 768      |\n",
      "|    mean_reward     | 45.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.74    |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 110397   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=48.20 +/- 127.18\n",
      "Episode length: 907.80 +/- 110.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 908      |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=63.30 +/- 108.86\n",
      "Episode length: 727.40 +/- 344.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 727      |\n",
      "|    mean_reward     | 63.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.29    |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 111216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=136.77 +/- 89.71\n",
      "Episode length: 751.60 +/- 269.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 752      |\n",
      "|    mean_reward     | 137      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=85.44 +/- 126.31\n",
      "Episode length: 803.60 +/- 270.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 85.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122500   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.61    |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 112086   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39m3e5\u001b[39m), callback\u001b[39m=\u001b[39meval_callback)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m: TD3Self,\n\u001b[0;32m    211\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    220\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TD3Self:\n\u001b[1;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    223\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    224\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    225\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    226\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    227\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    228\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    229\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    230\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    231\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    232\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:350\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    347\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    349\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 350\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[0;32m    351\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[0;32m    352\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[0;32m    353\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[0;32m    354\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    355\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[0;32m    356\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[0;32m    357\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    360\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:591\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    589\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m    590\u001b[0m \u001b[39m# Only stop training if return value is False, not when it is None.\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    592\u001b[0m     \u001b[39mreturn\u001b[39;00m RolloutReturn(num_collected_steps \u001b[39m*\u001b[39m env\u001b[39m.\u001b[39mnum_envs, num_collected_episodes, continue_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    594\u001b[0m \u001b[39m# Retrieve reward and episode length if using Monitor wrapper\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\callbacks.py:88\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39m# timesteps start at zero\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[1;32m---> 88\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\callbacks.py:435\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39m# Reset success rate buffer\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_success_buffer \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 435\u001b[0m episode_rewards, episode_lengths \u001b[39m=\u001b[39m evaluate_policy(\n\u001b[0;32m    436\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[0;32m    437\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_env,\n\u001b[0;32m    438\u001b[0m     n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    439\u001b[0m     render\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender,\n\u001b[0;32m    440\u001b[0m     deterministic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeterministic,\n\u001b[0;32m    441\u001b[0m     return_episode_rewards\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    442\u001b[0m     warn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn,\n\u001b[0;32m    443\u001b[0m     callback\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_success_callback,\n\u001b[0;32m    444\u001b[0m )\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluations_timesteps\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\evaluation.py:86\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     84\u001b[0m episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((env\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[1;32m---> 86\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(observations, state\u001b[39m=\u001b[39;49mstates, episode_start\u001b[39m=\u001b[39;49mepisode_starts, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[0;32m     87\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[0;32m     88\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\base_class.py:589\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m    570\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    571\u001b[0m     observation: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    574\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    575\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[0;32m    576\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 589\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\policies.py:333\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39m# TODO (GH/1): add support for RNN policies\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m# if state is None:\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39m#     state = self.initial_state\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39m# if episode_start is None:\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[39m#     episode_start = [False for _ in range(self.n_envs)]\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[39m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_training_mode(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 333\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_to_tensor(observation)\n\u001b[0;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    336\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(observation, deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\policies.py:252\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[39m# Add batch dimension if needed\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape)\n\u001b[1;32m--> 252\u001b[0m observation \u001b[39m=\u001b[39m obs_as_tensor(observation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    253\u001b[0m \u001b[39mreturn\u001b[39;00m observation, vectorized_env\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\utils.py:464\u001b[0m, in \u001b[0;36mobs_as_tensor\u001b[1;34m(obs, device)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39mMoves the observation to the given device.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39m:return: PyTorch tensor of the observation on a desired device.\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m th\u001b[39m.\u001b[39;49mas_tensor(obs)\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m    465\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    466\u001b[0m     \u001b[39mreturn\u001b[39;00m {key: th\u001b[39m.\u001b[39mas_tensor(_obs)\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m (key, _obs) \u001b[39min\u001b[39;00m obs\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(3e5), callback=eval_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
