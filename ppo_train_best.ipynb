{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "eval_env = make_vec_env(\"LunarLander-v2\")\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/discrete/ppo\",\n",
    "    log_path=\"./logs/discrete/ppo\",\n",
    "    eval_freq=1000,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"logs/discrete/ppo/best_model\", env=train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/ppo_lunar_tensorboard/PPO_7\n",
      "Eval num_timesteps=16000, episode_reward=276.21 +/- 11.66\n",
      "Episode length: 233.60 +/- 26.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 234      |\n",
      "|    mean_reward     | 276      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 228      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    fps             | 2082     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=277.07 +/- 18.17\n",
      "Episode length: 233.20 +/- 18.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 233          |\n",
      "|    mean_reward          | 277          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055377753 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.614       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.02         |\n",
      "|    n_updates            | 516          |\n",
      "|    policy_gradient_loss | 0.00114      |\n",
      "|    value_loss           | 6.67         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | 272      |\n",
      "| time/              |          |\n",
      "|    fps             | 1410     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=275.56 +/- 18.13\n",
      "Episode length: 211.80 +/- 7.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 212         |\n",
      "|    mean_reward          | 276         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002118506 |\n",
      "|    clip_fraction        | 0.013       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 90.9        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.000285   |\n",
      "|    value_loss           | 244         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 229      |\n",
      "|    ep_rew_mean     | 277      |\n",
      "| time/              |          |\n",
      "|    fps             | 1250     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=289.29 +/- 15.21\n",
      "Episode length: 221.00 +/- 21.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 221          |\n",
      "|    mean_reward          | 289          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034019428 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.97         |\n",
      "|    n_updates            | 524          |\n",
      "|    policy_gradient_loss | -0.000119    |\n",
      "|    value_loss           | 13.3         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 229      |\n",
      "|    ep_rew_mean     | 262      |\n",
      "| time/              |          |\n",
      "|    fps             | 1213     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=276.73 +/- 11.22\n",
      "Episode length: 220.80 +/- 24.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 221          |\n",
      "|    mean_reward          | 277          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033995523 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.566       |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.1         |\n",
      "|    n_updates            | 528          |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 222          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 227      |\n",
      "|    ep_rew_mean     | 274      |\n",
      "| time/              |          |\n",
      "|    fps             | 1182     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=284.31 +/- 28.42\n",
      "Episode length: 219.40 +/- 14.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 219          |\n",
      "|    mean_reward          | 284          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 96000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045902226 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.571       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 532          |\n",
      "|    policy_gradient_loss | 0.000367     |\n",
      "|    value_loss           | 15.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 277      |\n",
      "| time/              |          |\n",
      "|    fps             | 1154     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=278.49 +/- 24.25\n",
      "Episode length: 228.20 +/- 24.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 228         |\n",
      "|    mean_reward          | 278         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 112000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004871795 |\n",
      "|    clip_fraction        | 0.0536      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.4        |\n",
      "|    n_updates            | 536         |\n",
      "|    policy_gradient_loss | -2.41e-05   |\n",
      "|    value_loss           | 91.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    fps             | 1148     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=270.43 +/- 20.61\n",
      "Episode length: 214.40 +/- 15.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 214         |\n",
      "|    mean_reward          | 270         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 128000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004509568 |\n",
      "|    clip_fraction        | 0.0675      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.11        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | 0.00192     |\n",
      "|    value_loss           | 4.56        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | 284      |\n",
      "| time/              |          |\n",
      "|    fps             | 1150     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=278.11 +/- 20.67\n",
      "Episode length: 224.40 +/- 10.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 224          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038581714 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.586       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.97         |\n",
      "|    n_updates            | 544          |\n",
      "|    policy_gradient_loss | 0.00137      |\n",
      "|    value_loss           | 4.16         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 238      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    fps             | 1143     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=273.14 +/- 17.78\n",
      "Episode length: 220.00 +/- 8.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 220         |\n",
      "|    mean_reward          | 273         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004486789 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.05        |\n",
      "|    n_updates            | 548         |\n",
      "|    policy_gradient_loss | 0.000416    |\n",
      "|    value_loss           | 66.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 228      |\n",
      "|    ep_rew_mean     | 275      |\n",
      "| time/              |          |\n",
      "|    fps             | 1137     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=288.67 +/- 10.70\n",
      "Episode length: 224.40 +/- 22.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 224         |\n",
      "|    mean_reward          | 289         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 176000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004898564 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.35        |\n",
      "|    n_updates            | 552         |\n",
      "|    policy_gradient_loss | 0.00106     |\n",
      "|    value_loss           | 70          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 237      |\n",
      "|    ep_rew_mean     | 274      |\n",
      "| time/              |          |\n",
      "|    fps             | 1138     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=280.04 +/- 27.61\n",
      "Episode length: 219.60 +/- 9.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 220         |\n",
      "|    mean_reward          | 280         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 192000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005291719 |\n",
      "|    clip_fraction        | 0.0605      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.731       |\n",
      "|    n_updates            | 556         |\n",
      "|    policy_gradient_loss | 0.00186     |\n",
      "|    value_loss           | 24          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 225      |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 1138     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=282.43 +/- 15.76\n",
      "Episode length: 226.40 +/- 23.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 226          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 208000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042002425 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.561       |\n",
      "|    explained_variance   | 0.977        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.2         |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -7.95e-05    |\n",
      "|    value_loss           | 41.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 224      |\n",
      "|    ep_rew_mean     | 277      |\n",
      "| time/              |          |\n",
      "|    fps             | 1134     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 187      |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=267.00 +/- 9.45\n",
      "Episode length: 222.80 +/- 11.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 223         |\n",
      "|    mean_reward          | 267         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 224000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003607463 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 564         |\n",
      "|    policy_gradient_loss | 0.000399    |\n",
      "|    value_loss           | 59.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 224      |\n",
      "|    ep_rew_mean     | 272      |\n",
      "| time/              |          |\n",
      "|    fps             | 1132     |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=284.52 +/- 23.70\n",
      "Episode length: 217.00 +/- 15.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 217          |\n",
      "|    mean_reward          | 285          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041590724 |\n",
      "|    clip_fraction        | 0.0467       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.577       |\n",
      "|    explained_variance   | 0.963        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 568          |\n",
      "|    policy_gradient_loss | -6e-05       |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 227      |\n",
      "|    ep_rew_mean     | 285      |\n",
      "| time/              |          |\n",
      "|    fps             | 1116     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 220      |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=289.18 +/- 16.59\n",
      "Episode length: 227.00 +/- 18.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 227          |\n",
      "|    mean_reward          | 289          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 256000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040858625 |\n",
      "|    clip_fraction        | 0.0504       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.583       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77         |\n",
      "|    n_updates            | 572          |\n",
      "|    policy_gradient_loss | 0.000426     |\n",
      "|    value_loss           | 4.53         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 228      |\n",
      "|    ep_rew_mean     | 281      |\n",
      "| time/              |          |\n",
      "|    fps             | 1111     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=272.08 +/- 20.89\n",
      "Episode length: 209.00 +/- 14.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 209          |\n",
      "|    mean_reward          | 272          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 272000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035644532 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.586       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3          |\n",
      "|    n_updates            | 576          |\n",
      "|    policy_gradient_loss | 0.000967     |\n",
      "|    value_loss           | 3.44         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 224      |\n",
      "|    ep_rew_mean     | 281      |\n",
      "| time/              |          |\n",
      "|    fps             | 1097     |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 253      |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=286.10 +/- 21.56\n",
      "Episode length: 234.40 +/- 19.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 234          |\n",
      "|    mean_reward          | 286          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 288000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039606937 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.577       |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.73         |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | 0.000451     |\n",
      "|    value_loss           | 41           |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | 274      |\n",
      "| time/              |          |\n",
      "|    fps             | 1090     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=267.84 +/- 16.86\n",
      "Episode length: 218.00 +/- 19.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 218         |\n",
      "|    mean_reward          | 268         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 304000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003605195 |\n",
      "|    clip_fraction        | 0.0416      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.1        |\n",
      "|    n_updates            | 584         |\n",
      "|    policy_gradient_loss | 0.000869    |\n",
      "|    value_loss           | 98.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 218      |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 1091     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=273.60 +/- 19.29\n",
      "Episode length: 233.40 +/- 26.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 233          |\n",
      "|    mean_reward          | 274          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037600235 |\n",
      "|    clip_fraction        | 0.0411       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.23         |\n",
      "|    n_updates            | 588          |\n",
      "|    policy_gradient_loss | 0.000658     |\n",
      "|    value_loss           | 68           |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    fps             | 1092     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 299      |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=290.87 +/- 16.53\n",
      "Episode length: 218.60 +/- 11.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 219         |\n",
      "|    mean_reward          | 291         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005627748 |\n",
      "|    clip_fraction        | 0.0603      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 592         |\n",
      "|    policy_gradient_loss | 0.00149     |\n",
      "|    value_loss           | 3.68        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | 281      |\n",
      "| time/              |          |\n",
      "|    fps             | 1090     |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=272.20 +/- 12.15\n",
      "Episode length: 219.60 +/- 10.95\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 220          |\n",
      "|    mean_reward          | 272          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 352000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041009625 |\n",
      "|    clip_fraction        | 0.0465       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.562       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.778        |\n",
      "|    n_updates            | 596          |\n",
      "|    policy_gradient_loss | 0.00147      |\n",
      "|    value_loss           | 2.71         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 215      |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    fps             | 1090     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=287.66 +/- 10.15\n",
      "Episode length: 241.20 +/- 24.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 241          |\n",
      "|    mean_reward          | 288          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 368000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036082752 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.57        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.974        |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | 0.000741     |\n",
      "|    value_loss           | 7.7          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 224      |\n",
      "|    ep_rew_mean     | 276      |\n",
      "| time/              |          |\n",
      "|    fps             | 1087     |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 346      |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=290.22 +/- 15.63\n",
      "Episode length: 225.00 +/- 31.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | 290          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 384000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028736687 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.554       |\n",
      "|    explained_variance   | 0.977        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.8         |\n",
      "|    n_updates            | 604          |\n",
      "|    policy_gradient_loss | 0.000798     |\n",
      "|    value_loss           | 54.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 226      |\n",
      "|    ep_rew_mean     | 272      |\n",
      "| time/              |          |\n",
      "|    fps             | 1087     |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 361      |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=278.28 +/- 22.19\n",
      "Episode length: 226.80 +/- 16.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 227          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019219494 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.553       |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 251          |\n",
      "|    n_updates            | 608          |\n",
      "|    policy_gradient_loss | 0.000303     |\n",
      "|    value_loss           | 137          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 226      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    fps             | 1088     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 376      |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=276.26 +/- 16.96\n",
      "Episode length: 215.60 +/- 13.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 216          |\n",
      "|    mean_reward          | 276          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 416000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053620827 |\n",
      "|    clip_fraction        | 0.0629       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.46         |\n",
      "|    n_updates            | 612          |\n",
      "|    policy_gradient_loss | 0.000851     |\n",
      "|    value_loss           | 98.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 223      |\n",
      "|    ep_rew_mean     | 276      |\n",
      "| time/              |          |\n",
      "|    fps             | 1087     |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 391      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=286.75 +/- 25.27\n",
      "Episode length: 214.20 +/- 22.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 214        |\n",
      "|    mean_reward          | 287        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 432000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00520336 |\n",
      "|    clip_fraction        | 0.0586     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.551     |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3          |\n",
      "|    n_updates            | 616        |\n",
      "|    policy_gradient_loss | -6.4e-05   |\n",
      "|    value_loss           | 44.7       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 234      |\n",
      "|    ep_rew_mean     | 276      |\n",
      "| time/              |          |\n",
      "|    fps             | 1089     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 406      |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=262.13 +/- 15.17\n",
      "Episode length: 229.60 +/- 20.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 230          |\n",
      "|    mean_reward          | 262          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 448000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054659536 |\n",
      "|    clip_fraction        | 0.065        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.561       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.772        |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | 0.00139      |\n",
      "|    value_loss           | 3.22         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 217      |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 1091     |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 420      |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=285.11 +/- 19.44\n",
      "Episode length: 223.60 +/- 16.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 224          |\n",
      "|    mean_reward          | 285          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 464000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044325925 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.57        |\n",
      "|    explained_variance   | 0.978        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 281          |\n",
      "|    n_updates            | 624          |\n",
      "|    policy_gradient_loss | 0.000654     |\n",
      "|    value_loss           | 69.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    fps             | 1090     |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 435      |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=265.15 +/- 25.47\n",
      "Episode length: 222.40 +/- 24.78\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 222        |\n",
      "|    mean_reward          | 265        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 480000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00405818 |\n",
      "|    clip_fraction        | 0.0447     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.563     |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 136        |\n",
      "|    n_updates            | 628        |\n",
      "|    policy_gradient_loss | 0.00127    |\n",
      "|    value_loss           | 85         |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 222      |\n",
      "|    ep_rew_mean     | 275      |\n",
      "| time/              |          |\n",
      "|    fps             | 1091     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 450      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=296.02 +/- 14.78\n",
      "Episode length: 234.60 +/- 32.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 235         |\n",
      "|    mean_reward          | 296         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 496000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004760435 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.3        |\n",
      "|    n_updates            | 632         |\n",
      "|    policy_gradient_loss | 0.000966    |\n",
      "|    value_loss           | 41.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 223      |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    fps             | 1091     |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 465      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=282.15 +/- 20.85\n",
      "Episode length: 233.40 +/- 20.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 233          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040484164 |\n",
      "|    clip_fraction        | 0.0534       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.562       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18         |\n",
      "|    n_updates            | 636          |\n",
      "|    policy_gradient_loss | 0.000897     |\n",
      "|    value_loss           | 2.93         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 217      |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 1091     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 480      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=285.87 +/- 16.80\n",
      "Episode length: 211.20 +/- 14.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 211          |\n",
      "|    mean_reward          | 286          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 528000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034097077 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.572       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.08         |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | 0.00164      |\n",
      "|    value_loss           | 22.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 222      |\n",
      "|    ep_rew_mean     | 273      |\n",
      "| time/              |          |\n",
      "|    fps             | 1092     |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 494      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=285.82 +/- 11.48\n",
      "Episode length: 222.00 +/- 24.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 222          |\n",
      "|    mean_reward          | 286          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040542725 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.56        |\n",
      "|    explained_variance   | 0.964        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.9         |\n",
      "|    n_updates            | 644          |\n",
      "|    policy_gradient_loss | 0.000504     |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 271      |\n",
      "| time/              |          |\n",
      "|    fps             | 1092     |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 509      |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=285.34 +/- 19.13\n",
      "Episode length: 245.00 +/- 33.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 245         |\n",
      "|    mean_reward          | 285         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004565082 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.42        |\n",
      "|    n_updates            | 648         |\n",
      "|    policy_gradient_loss | 0.000289    |\n",
      "|    value_loss           | 166         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    fps             | 1093     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 524      |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=277.91 +/- 14.14\n",
      "Episode length: 214.40 +/- 29.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 214          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 576000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050983042 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.554       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.42         |\n",
      "|    n_updates            | 652          |\n",
      "|    policy_gradient_loss | 0.000798     |\n",
      "|    value_loss           | 13.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    fps             | 1094     |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 538      |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=282.95 +/- 11.47\n",
      "Episode length: 224.40 +/- 18.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 224          |\n",
      "|    mean_reward          | 283          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 592000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043982225 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.558       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.41         |\n",
      "|    n_updates            | 656          |\n",
      "|    policy_gradient_loss | 0.00105      |\n",
      "|    value_loss           | 7.45         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 227      |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    fps             | 1094     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 553      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=273.24 +/- 15.29\n",
      "Episode length: 217.40 +/- 22.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 217         |\n",
      "|    mean_reward          | 273         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 608000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006864817 |\n",
      "|    clip_fraction        | 0.0618      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.00051     |\n",
      "|    value_loss           | 77.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 222      |\n",
      "|    ep_rew_mean     | 277      |\n",
      "| time/              |          |\n",
      "|    fps             | 1096     |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 567      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=294.68 +/- 14.92\n",
      "Episode length: 219.20 +/- 22.73\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 219        |\n",
      "|    mean_reward          | 295        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 624000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00523708 |\n",
      "|    clip_fraction        | 0.0671     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.562     |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.13       |\n",
      "|    n_updates            | 664        |\n",
      "|    policy_gradient_loss | 0.00172    |\n",
      "|    value_loss           | 25.9       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 224      |\n",
      "|    ep_rew_mean     | 277      |\n",
      "| time/              |          |\n",
      "|    fps             | 1096     |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 582      |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=271.54 +/- 11.84\n",
      "Episode length: 208.60 +/- 12.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 209          |\n",
      "|    mean_reward          | 272          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 640000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037226032 |\n",
      "|    clip_fraction        | 0.041        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.549       |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.879        |\n",
      "|    n_updates            | 668          |\n",
      "|    policy_gradient_loss | 0.000344     |\n",
      "|    value_loss           | 72.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 227      |\n",
      "|    ep_rew_mean     | 283      |\n",
      "| time/              |          |\n",
      "|    fps             | 1097     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 597      |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=275.74 +/- 7.17\n",
      "Episode length: 217.20 +/- 13.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 217          |\n",
      "|    mean_reward          | 276          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 656000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045685144 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.551       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.839        |\n",
      "|    n_updates            | 672          |\n",
      "|    policy_gradient_loss | 0.000249     |\n",
      "|    value_loss           | 1.93         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 227      |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    fps             | 1097     |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 611      |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=279.92 +/- 12.30\n",
      "Episode length: 221.00 +/- 19.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 221         |\n",
      "|    mean_reward          | 280         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 672000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003918827 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.71        |\n",
      "|    n_updates            | 676         |\n",
      "|    policy_gradient_loss | -0.000347   |\n",
      "|    value_loss           | 30.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=273.64 +/- 24.70\n",
      "Episode length: 235.00 +/- 8.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 235      |\n",
      "|    mean_reward     | 274      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | 282      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 628      |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=276.03 +/- 22.84\n",
      "Episode length: 226.80 +/- 27.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 227          |\n",
      "|    mean_reward          | 276          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 704000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043634987 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.554       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2          |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | 0.000485     |\n",
      "|    value_loss           | 2.85         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 282      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 643      |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=272.64 +/- 19.78\n",
      "Episode length: 225.20 +/- 22.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 225         |\n",
      "|    mean_reward          | 273         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004015688 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.807       |\n",
      "|    n_updates            | 684         |\n",
      "|    policy_gradient_loss | 7.49e-05    |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | 284      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 658      |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=278.11 +/- 13.73\n",
      "Episode length: 218.60 +/- 26.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 219          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 736000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041844836 |\n",
      "|    clip_fraction        | 0.0551       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.543        |\n",
      "|    n_updates            | 688          |\n",
      "|    policy_gradient_loss | 0.000451     |\n",
      "|    value_loss           | 1.52         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | 284      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 672      |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=284.06 +/- 15.75\n",
      "Episode length: 216.80 +/- 7.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 217          |\n",
      "|    mean_reward          | 284          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 752000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036969972 |\n",
      "|    clip_fraction        | 0.0411       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.527       |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 139          |\n",
      "|    n_updates            | 692          |\n",
      "|    policy_gradient_loss | -0.000409    |\n",
      "|    value_loss           | 70.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    fps             | 1096     |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 687      |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=267.99 +/- 12.39\n",
      "Episode length: 229.00 +/- 22.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 229          |\n",
      "|    mean_reward          | 268          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 768000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051076123 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.978        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.38         |\n",
      "|    n_updates            | 696          |\n",
      "|    policy_gradient_loss | 0.000555     |\n",
      "|    value_loss           | 72.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 702      |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=268.76 +/- 12.18\n",
      "Episode length: 213.40 +/- 13.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 213          |\n",
      "|    mean_reward          | 269          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 784000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030148653 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.745        |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | 0.000886     |\n",
      "|    value_loss           | 45.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 222      |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 717      |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=282.30 +/- 13.42\n",
      "Episode length: 243.80 +/- 30.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 244          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 800000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033710026 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.546       |\n",
      "|    explained_variance   | 0.95         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.12         |\n",
      "|    n_updates            | 704          |\n",
      "|    policy_gradient_loss | 0.000112     |\n",
      "|    value_loss           | 77.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 218      |\n",
      "|    ep_rew_mean     | 275      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 733      |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=272.13 +/- 12.71\n",
      "Episode length: 223.00 +/- 27.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 223         |\n",
      "|    mean_reward          | 272         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 816000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003541085 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.4         |\n",
      "|    n_updates            | 708         |\n",
      "|    policy_gradient_loss | 0.00122     |\n",
      "|    value_loss           | 83.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 223      |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 1096     |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 747      |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=288.21 +/- 12.19\n",
      "Episode length: 223.40 +/- 33.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 223         |\n",
      "|    mean_reward          | 288         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 832000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004230392 |\n",
      "|    clip_fraction        | 0.0602      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.66        |\n",
      "|    n_updates            | 712         |\n",
      "|    policy_gradient_loss | 0.000713    |\n",
      "|    value_loss           | 70.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 277      |\n",
      "| time/              |          |\n",
      "|    fps             | 1096     |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 761      |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=297.15 +/- 12.02\n",
      "Episode length: 226.60 +/- 24.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 227         |\n",
      "|    mean_reward          | 297         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 848000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003926388 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.943       |\n",
      "|    n_updates            | 716         |\n",
      "|    policy_gradient_loss | 0.00129     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    fps             | 1096     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 777      |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=275.53 +/- 17.19\n",
      "Episode length: 230.20 +/- 22.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 230          |\n",
      "|    mean_reward          | 276          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 864000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037328298 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.537       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05         |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -4.67e-07    |\n",
      "|    value_loss           | 6.57         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 228      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    fps             | 1096     |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 792      |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=284.97 +/- 9.68\n",
      "Episode length: 226.60 +/- 19.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 227         |\n",
      "|    mean_reward          | 285         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003984905 |\n",
      "|    clip_fraction        | 0.0524      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 317         |\n",
      "|    n_updates            | 724         |\n",
      "|    policy_gradient_loss | -2.28e-06   |\n",
      "|    value_loss           | 72.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | 275      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 807      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=278.65 +/- 17.12\n",
      "Episode length: 273.60 +/- 133.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 274         |\n",
      "|    mean_reward          | 279         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 896000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004833025 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.58        |\n",
      "|    n_updates            | 728         |\n",
      "|    policy_gradient_loss | 0.000964    |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 231      |\n",
      "|    ep_rew_mean     | 275      |\n",
      "| time/              |          |\n",
      "|    fps             | 1094     |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 823      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=276.05 +/- 20.31\n",
      "Episode length: 224.20 +/- 23.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 224          |\n",
      "|    mean_reward          | 276          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 912000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049868226 |\n",
      "|    clip_fraction        | 0.0735       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.15         |\n",
      "|    n_updates            | 732          |\n",
      "|    policy_gradient_loss | 0.00114      |\n",
      "|    value_loss           | 3.18         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 222      |\n",
      "|    ep_rew_mean     | 282      |\n",
      "| time/              |          |\n",
      "|    fps             | 1094     |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 837      |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=294.64 +/- 13.89\n",
      "Episode length: 219.60 +/- 21.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 220         |\n",
      "|    mean_reward          | 295         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 928000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004666071 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.38        |\n",
      "|    n_updates            | 736         |\n",
      "|    policy_gradient_loss | 0.000546    |\n",
      "|    value_loss           | 10.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 230      |\n",
      "|    ep_rew_mean     | 282      |\n",
      "| time/              |          |\n",
      "|    fps             | 1094     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 853      |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=282.39 +/- 18.51\n",
      "Episode length: 232.40 +/- 15.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 232          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 944000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042913854 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.534       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.98         |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | 0.00148      |\n",
      "|    value_loss           | 3.93         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 229      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    fps             | 1094     |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 868      |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=282.95 +/- 13.84\n",
      "Episode length: 225.40 +/- 26.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 225         |\n",
      "|    mean_reward          | 283         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005763038 |\n",
      "|    clip_fraction        | 0.0299      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 252         |\n",
      "|    n_updates            | 744         |\n",
      "|    policy_gradient_loss | 0.000118    |\n",
      "|    value_loss           | 138         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 283      |\n",
      "| time/              |          |\n",
      "|    fps             | 1094     |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 883      |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=277.60 +/- 14.64\n",
      "Episode length: 216.60 +/- 11.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 217         |\n",
      "|    mean_reward          | 278         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 976000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003413796 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.935       |\n",
      "|    n_updates            | 748         |\n",
      "|    policy_gradient_loss | 0.00132     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 226      |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 897      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=280.37 +/- 10.05\n",
      "Episode length: 233.60 +/- 26.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 234          |\n",
      "|    mean_reward          | 280          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 992000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036417139 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.548       |\n",
      "|    explained_variance   | 0.935        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 207          |\n",
      "|    n_updates            | 752          |\n",
      "|    policy_gradient_loss | -3.02e-05    |\n",
      "|    value_loss           | 197          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | 276      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 912      |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=281.86 +/- 17.90\n",
      "Episode length: 224.20 +/- 14.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 224         |\n",
      "|    mean_reward          | 282         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1008000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003864599 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.53        |\n",
      "|    n_updates            | 756         |\n",
      "|    policy_gradient_loss | 0.000184    |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 217      |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    fps             | 1095     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 927      |\n",
      "|    total_timesteps | 1015808  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x23fdf4b9c90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e6), callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"data/policies/LunarLander-v2#ppo#train_best\")\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
