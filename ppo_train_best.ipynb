{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = make_vec_env(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2', n_envs=16)\n",
    "eval_env = Monitor(gym.make(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2'))\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo',\n",
    "    log_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo',\n",
    "    eval_freq=1000,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo/best_model', env=train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/ppo_lunarcontinuous_tensorboard/PPO_5\n",
      "Eval num_timesteps=16000, episode_reward=250.99 +/- 60.91\n",
      "Episode length: 221.40 +/- 22.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 221      |\n",
      "|    mean_reward     | 251      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 139      |\n",
      "| time/              |          |\n",
      "|    fps             | 1472     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=278.09 +/- 23.56\n",
      "Episode length: 229.30 +/- 11.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 229          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027911183 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.972        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.8         |\n",
      "|    n_updates            | 576          |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    std                  | 0.876        |\n",
      "|    value_loss           | 59.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 1314     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=289.91 +/- 16.55\n",
      "Episode length: 227.00 +/- 16.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 227          |\n",
      "|    mean_reward          | 290          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041805427 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.639        |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | 0.00103      |\n",
      "|    std                  | 0.881        |\n",
      "|    value_loss           | 3.66         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 158      |\n",
      "| time/              |          |\n",
      "|    fps             | 1297     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=275.99 +/- 28.64\n",
      "Episode length: 228.10 +/- 16.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 228          |\n",
      "|    mean_reward          | 276          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030937851 |\n",
      "|    clip_fraction        | 0.0281       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.579        |\n",
      "|    n_updates            | 584          |\n",
      "|    policy_gradient_loss | 0.000948     |\n",
      "|    std                  | 0.873        |\n",
      "|    value_loss           | 20.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 1275     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=296.98 +/- 14.17\n",
      "Episode length: 228.60 +/- 26.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 229          |\n",
      "|    mean_reward          | 297          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027006571 |\n",
      "|    clip_fraction        | 0.0131       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.963        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.06         |\n",
      "|    n_updates            | 588          |\n",
      "|    policy_gradient_loss | -0.000939    |\n",
      "|    std                  | 0.878        |\n",
      "|    value_loss           | 53.6         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 1267     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=282.30 +/- 24.42\n",
      "Episode length: 229.70 +/- 21.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 230         |\n",
      "|    mean_reward          | 282         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002931525 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.5        |\n",
      "|    n_updates            | 592         |\n",
      "|    policy_gradient_loss | 8.72e-05    |\n",
      "|    std                  | 0.878       |\n",
      "|    value_loss           | 51.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 1251     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=287.48 +/- 15.69\n",
      "Episode length: 215.60 +/- 17.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 216         |\n",
      "|    mean_reward          | 287         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 112000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006818806 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.13        |\n",
      "|    n_updates            | 596         |\n",
      "|    policy_gradient_loss | 0.000657    |\n",
      "|    std                  | 0.877       |\n",
      "|    value_loss           | 40.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 1239     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=287.93 +/- 24.52\n",
      "Episode length: 220.50 +/- 11.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 220          |\n",
      "|    mean_reward          | 288          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036245924 |\n",
      "|    clip_fraction        | 0.0475       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.977        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.12         |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | 0.000347     |\n",
      "|    std                  | 0.877        |\n",
      "|    value_loss           | 43.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 1237     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=287.97 +/- 17.27\n",
      "Episode length: 228.90 +/- 19.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 229          |\n",
      "|    mean_reward          | 288          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048728446 |\n",
      "|    clip_fraction        | 0.0403       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.74         |\n",
      "|    n_updates            | 604          |\n",
      "|    policy_gradient_loss | -3.49e-05    |\n",
      "|    std                  | 0.873        |\n",
      "|    value_loss           | 32.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 1229     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=278.26 +/- 10.28\n",
      "Episode length: 216.30 +/- 11.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 216         |\n",
      "|    mean_reward          | 278         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004402977 |\n",
      "|    clip_fraction        | 0.0365      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.51        |\n",
      "|    n_updates            | 608         |\n",
      "|    policy_gradient_loss | 0.00064     |\n",
      "|    std                  | 0.865       |\n",
      "|    value_loss           | 39.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 1228     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=282.37 +/- 17.12\n",
      "Episode length: 225.10 +/- 14.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044730427 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 82.3         |\n",
      "|    n_updates            | 612          |\n",
      "|    policy_gradient_loss | -6.39e-06    |\n",
      "|    std                  | 0.879        |\n",
      "|    value_loss           | 51.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 1230     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=245.61 +/- 84.81\n",
      "Episode length: 217.00 +/- 43.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 217          |\n",
      "|    mean_reward          | 246          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 192000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067775664 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.31         |\n",
      "|    n_updates            | 616          |\n",
      "|    policy_gradient_loss | 0.0015       |\n",
      "|    std                  | 0.885        |\n",
      "|    value_loss           | 29.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 1233     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 159      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=267.14 +/- 14.44\n",
      "Episode length: 216.00 +/- 16.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 216          |\n",
      "|    mean_reward          | 267          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 208000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045967754 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.26         |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | 0.0011       |\n",
      "|    std                  | 0.881        |\n",
      "|    value_loss           | 26.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 1226     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=270.63 +/- 19.70\n",
      "Episode length: 212.10 +/- 14.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 212          |\n",
      "|    mean_reward          | 271          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028920295 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68         |\n",
      "|    n_updates            | 624          |\n",
      "|    policy_gradient_loss | 0.000722     |\n",
      "|    std                  | 0.879        |\n",
      "|    value_loss           | 41.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 1226     |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 187      |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=276.71 +/- 18.00\n",
      "Episode length: 227.90 +/- 16.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 228          |\n",
      "|    mean_reward          | 277          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059640286 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.27         |\n",
      "|    n_updates            | 628          |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    std                  | 0.879        |\n",
      "|    value_loss           | 20           |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 1219     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 201      |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=286.97 +/- 16.48\n",
      "Episode length: 232.00 +/- 18.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 232          |\n",
      "|    mean_reward          | 287          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 256000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034926354 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.13         |\n",
      "|    n_updates            | 632          |\n",
      "|    policy_gradient_loss | 2.57e-05     |\n",
      "|    std                  | 0.881        |\n",
      "|    value_loss           | 39           |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 1217     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 215      |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=287.79 +/- 19.92\n",
      "Episode length: 222.50 +/- 10.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 222         |\n",
      "|    mean_reward          | 288         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 272000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004350188 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 636         |\n",
      "|    policy_gradient_loss | 0.000203    |\n",
      "|    std                  | 0.873       |\n",
      "|    value_loss           | 43.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 1217     |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 228      |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=274.29 +/- 20.80\n",
      "Episode length: 227.60 +/- 19.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 228          |\n",
      "|    mean_reward          | 274          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 288000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030407337 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.31         |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | 8.95e-05     |\n",
      "|    std                  | 0.871        |\n",
      "|    value_loss           | 61           |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 1216     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 242      |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=265.45 +/- 25.16\n",
      "Episode length: 222.20 +/- 18.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 222          |\n",
      "|    mean_reward          | 265          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 304000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052060336 |\n",
      "|    clip_fraction        | 0.0581       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.9         |\n",
      "|    n_updates            | 644          |\n",
      "|    policy_gradient_loss | 0.00149      |\n",
      "|    std                  | 0.876        |\n",
      "|    value_loss           | 26.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 1215     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 256      |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=278.97 +/- 16.09\n",
      "Episode length: 214.70 +/- 14.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 215         |\n",
      "|    mean_reward          | 279         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004035991 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.02        |\n",
      "|    n_updates            | 648         |\n",
      "|    policy_gradient_loss | 0.000301    |\n",
      "|    std                  | 0.883       |\n",
      "|    value_loss           | 29.7        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 1211     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=272.34 +/- 11.00\n",
      "Episode length: 231.80 +/- 13.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 232          |\n",
      "|    mean_reward          | 272          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 336000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061246445 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.94         |\n",
      "|    n_updates            | 652          |\n",
      "|    policy_gradient_loss | 0.000635     |\n",
      "|    std                  | 0.884        |\n",
      "|    value_loss           | 36.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 1209     |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 284      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=281.80 +/- 16.09\n",
      "Episode length: 226.20 +/- 12.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 226          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 352000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047189137 |\n",
      "|    clip_fraction        | 0.0554       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.56        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.303        |\n",
      "|    n_updates            | 656          |\n",
      "|    policy_gradient_loss | 0.000244     |\n",
      "|    std                  | 0.885        |\n",
      "|    value_loss           | 1.51         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 1208     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 298      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=274.38 +/- 16.19\n",
      "Episode length: 225.10 +/- 15.29\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | 274          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 368000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032803472 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.56        |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.71         |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | 0.000419     |\n",
      "|    std                  | 0.88         |\n",
      "|    value_loss           | 20.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 957      |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    fps             | 1205     |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 312      |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=278.93 +/- 23.81\n",
      "Episode length: 229.40 +/- 16.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 229         |\n",
      "|    mean_reward          | 279         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 384000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005178622 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.402       |\n",
      "|    n_updates            | 664         |\n",
      "|    policy_gradient_loss | -0.000615   |\n",
      "|    std                  | 0.877       |\n",
      "|    value_loss           | 1.19        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 957      |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    fps             | 1203     |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 326      |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=270.60 +/- 16.46\n",
      "Episode length: 215.10 +/- 9.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 215         |\n",
      "|    mean_reward          | 271         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006056713 |\n",
      "|    clip_fraction        | 0.055       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.784       |\n",
      "|    n_updates            | 668         |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    std                  | 0.866       |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 939      |\n",
      "|    ep_rew_mean     | 154      |\n",
      "| time/              |          |\n",
      "|    fps             | 1203     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 340      |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=283.93 +/- 17.08\n",
      "Episode length: 226.70 +/- 10.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 227         |\n",
      "|    mean_reward          | 284         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 416000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004014044 |\n",
      "|    clip_fraction        | 0.0198      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.75        |\n",
      "|    n_updates            | 672         |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    std                  | 0.871       |\n",
      "|    value_loss           | 70.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 939      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 1204     |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 353      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=274.85 +/- 17.66\n",
      "Episode length: 230.20 +/- 18.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 230          |\n",
      "|    mean_reward          | 275          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 432000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038738842 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.8         |\n",
      "|    n_updates            | 676          |\n",
      "|    policy_gradient_loss | 0.000667     |\n",
      "|    std                  | 0.879        |\n",
      "|    value_loss           | 16.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 931      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 1205     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 367      |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=278.28 +/- 12.89\n",
      "Episode length: 224.80 +/- 11.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 448000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044597154 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.2         |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.000138    |\n",
      "|    std                  | 0.877        |\n",
      "|    value_loss           | 35.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 931      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 1203     |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 381      |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=286.47 +/- 22.11\n",
      "Episode length: 225.90 +/- 10.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 226         |\n",
      "|    mean_reward          | 286         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 464000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005962476 |\n",
      "|    clip_fraction        | 0.0624      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.56       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.29        |\n",
      "|    n_updates            | 684         |\n",
      "|    policy_gradient_loss | -0.000382   |\n",
      "|    std                  | 0.881       |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 940      |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    fps             | 1202     |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 395      |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=283.60 +/- 17.84\n",
      "Episode length: 225.20 +/- 17.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | 284          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045042452 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.264        |\n",
      "|    n_updates            | 688          |\n",
      "|    policy_gradient_loss | 0.000613     |\n",
      "|    std                  | 0.86         |\n",
      "|    value_loss           | 1.29         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 940      |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    fps             | 1202     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 408      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=279.84 +/- 14.45\n",
      "Episode length: 225.00 +/- 11.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | 280          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 496000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058382014 |\n",
      "|    clip_fraction        | 0.0512       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.142        |\n",
      "|    n_updates            | 692          |\n",
      "|    policy_gradient_loss | 0.000386     |\n",
      "|    std                  | 0.866        |\n",
      "|    value_loss           | 0.89         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 974      |\n",
      "|    ep_rew_mean     | 162      |\n",
      "| time/              |          |\n",
      "|    fps             | 1202     |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 422      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=281.53 +/- 12.83\n",
      "Episode length: 220.80 +/- 12.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 221          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034328795 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.37         |\n",
      "|    n_updates            | 696          |\n",
      "|    policy_gradient_loss | 0.000268     |\n",
      "|    std                  | 0.856        |\n",
      "|    value_loss           | 0.573        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 975      |\n",
      "|    ep_rew_mean     | 162      |\n",
      "| time/              |          |\n",
      "|    fps             | 1201     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 436      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=279.90 +/- 23.63\n",
      "Episode length: 221.30 +/- 15.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 221         |\n",
      "|    mean_reward          | 280         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 528000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003377508 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00054    |\n",
      "|    std                  | 0.848       |\n",
      "|    value_loss           | 12.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 974      |\n",
      "|    ep_rew_mean     | 164      |\n",
      "| time/              |          |\n",
      "|    fps             | 1200     |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 450      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=265.10 +/- 19.37\n",
      "Episode length: 214.50 +/- 8.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 214          |\n",
      "|    mean_reward          | 265          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057088477 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.26         |\n",
      "|    n_updates            | 704          |\n",
      "|    policy_gradient_loss | -0.000837    |\n",
      "|    std                  | 0.844        |\n",
      "|    value_loss           | 23.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 966      |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    fps             | 1200     |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 464      |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=284.80 +/- 10.59\n",
      "Episode length: 211.30 +/- 19.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 211          |\n",
      "|    mean_reward          | 285          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 560000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030746206 |\n",
      "|    clip_fraction        | 0.0282       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.739        |\n",
      "|    n_updates            | 708          |\n",
      "|    policy_gradient_loss | 0.000201     |\n",
      "|    std                  | 0.841        |\n",
      "|    value_loss           | 19.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 949      |\n",
      "|    ep_rew_mean     | 158      |\n",
      "| time/              |          |\n",
      "|    fps             | 1202     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 477      |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=274.23 +/- 23.39\n",
      "Episode length: 232.30 +/- 24.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 232          |\n",
      "|    mean_reward          | 274          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 576000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027719282 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.471        |\n",
      "|    n_updates            | 712          |\n",
      "|    policy_gradient_loss | -0.000296    |\n",
      "|    std                  | 0.849        |\n",
      "|    value_loss           | 29.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 932      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 1201     |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 490      |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=287.68 +/- 19.03\n",
      "Episode length: 227.20 +/- 15.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 227         |\n",
      "|    mean_reward          | 288         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 592000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004345118 |\n",
      "|    clip_fraction        | 0.0267      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 716         |\n",
      "|    policy_gradient_loss | 0.000114    |\n",
      "|    std                  | 0.845       |\n",
      "|    value_loss           | 26.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 932      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 1200     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 504      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=261.88 +/- 64.97\n",
      "Episode length: 213.50 +/- 21.99\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 214          |\n",
      "|    mean_reward          | 262          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 608000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050184675 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.441        |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | 0.000905     |\n",
      "|    std                  | 0.832        |\n",
      "|    value_loss           | 1.07         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 931      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 1201     |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 518      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=287.60 +/- 13.02\n",
      "Episode length: 224.80 +/- 21.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | 288          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037809024 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.54         |\n",
      "|    n_updates            | 724          |\n",
      "|    policy_gradient_loss | -0.000207    |\n",
      "|    std                  | 0.822        |\n",
      "|    value_loss           | 30.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 949      |\n",
      "|    ep_rew_mean     | 158      |\n",
      "| time/              |          |\n",
      "|    fps             | 1202     |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 531      |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=238.78 +/- 62.26\n",
      "Episode length: 235.60 +/- 20.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 236          |\n",
      "|    mean_reward          | 239          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 640000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045969617 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.43        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.375        |\n",
      "|    n_updates            | 728          |\n",
      "|    policy_gradient_loss | 0.000451     |\n",
      "|    std                  | 0.819        |\n",
      "|    value_loss           | 1.11         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 957      |\n",
      "|    ep_rew_mean     | 162      |\n",
      "| time/              |          |\n",
      "|    fps             | 1201     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 545      |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=263.25 +/- 69.87\n",
      "Episode length: 222.80 +/- 22.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 223         |\n",
      "|    mean_reward          | 263         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 656000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004904635 |\n",
      "|    clip_fraction        | 0.0565      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.43       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.42        |\n",
      "|    n_updates            | 732         |\n",
      "|    policy_gradient_loss | -0.000145   |\n",
      "|    std                  | 0.82        |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 966      |\n",
      "|    ep_rew_mean     | 164      |\n",
      "| time/              |          |\n",
      "|    fps             | 1204     |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 557      |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=272.79 +/- 18.17\n",
      "Episode length: 226.30 +/- 24.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 226          |\n",
      "|    mean_reward          | 273          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 672000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031713443 |\n",
      "|    clip_fraction        | 0.0226       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.43        |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.591        |\n",
      "|    n_updates            | 736          |\n",
      "|    policy_gradient_loss | 4.47e-05     |\n",
      "|    std                  | 0.818        |\n",
      "|    value_loss           | 13.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=284.02 +/- 17.32\n",
      "Episode length: 230.50 +/- 23.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 230      |\n",
      "|    mean_reward     | 284      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 974      |\n",
      "|    ep_rew_mean     | 165      |\n",
      "| time/              |          |\n",
      "|    fps             | 1202     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 572      |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=286.27 +/- 17.92\n",
      "Episode length: 228.30 +/- 29.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 228          |\n",
      "|    mean_reward          | 286          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 704000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049835695 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.42        |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.4         |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.000565    |\n",
      "|    std                  | 0.815        |\n",
      "|    value_loss           | 14.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 957      |\n",
      "|    ep_rew_mean     | 164      |\n",
      "| time/              |          |\n",
      "|    fps             | 1201     |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 586      |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=289.49 +/- 23.13\n",
      "Episode length: 215.10 +/- 15.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 215          |\n",
      "|    mean_reward          | 289          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 720000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031477385 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.41        |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.607        |\n",
      "|    n_updates            | 744          |\n",
      "|    policy_gradient_loss | 0.000512     |\n",
      "|    std                  | 0.809        |\n",
      "|    value_loss           | 39.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 957      |\n",
      "|    ep_rew_mean     | 165      |\n",
      "| time/              |          |\n",
      "|    fps             | 1201     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 599      |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=294.80 +/- 24.60\n",
      "Episode length: 227.70 +/- 15.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 228          |\n",
      "|    mean_reward          | 295          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 736000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042200917 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.4         |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.5          |\n",
      "|    n_updates            | 748          |\n",
      "|    policy_gradient_loss | 0.000703     |\n",
      "|    std                  | 0.806        |\n",
      "|    value_loss           | 22.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 948      |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    fps             | 1202     |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 612      |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=282.61 +/- 18.49\n",
      "Episode length: 242.20 +/- 41.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 242         |\n",
      "|    mean_reward          | 283         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 752000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003629452 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.879       |\n",
      "|    n_updates            | 752         |\n",
      "|    policy_gradient_loss | 0.000216    |\n",
      "|    std                  | 0.798       |\n",
      "|    value_loss           | 18.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 939      |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    fps             | 1202     |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 626      |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=246.23 +/- 68.22\n",
      "Episode length: 227.00 +/- 18.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 227         |\n",
      "|    mean_reward          | 246         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 768000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004357877 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 756         |\n",
      "|    policy_gradient_loss | 0.00106     |\n",
      "|    std                  | 0.801       |\n",
      "|    value_loss           | 22.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 159      |\n",
      "| time/              |          |\n",
      "|    fps             | 1203     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 639      |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=284.35 +/- 10.13\n",
      "Episode length: 233.80 +/- 29.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 234          |\n",
      "|    mean_reward          | 284          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 784000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025169384 |\n",
      "|    clip_fraction        | 0.0109       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.39        |\n",
      "|    explained_variance   | 0.978        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.4         |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.000527    |\n",
      "|    std                  | 0.803        |\n",
      "|    value_loss           | 43.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    fps             | 1203     |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 653      |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=280.06 +/- 19.65\n",
      "Episode length: 221.80 +/- 29.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 222          |\n",
      "|    mean_reward          | 280          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 800000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034270769 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.4         |\n",
      "|    explained_variance   | 0.955        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.92         |\n",
      "|    n_updates            | 764          |\n",
      "|    policy_gradient_loss | -0.000792    |\n",
      "|    std                  | 0.806        |\n",
      "|    value_loss           | 76.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    fps             | 1204     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 666      |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=289.32 +/- 15.20\n",
      "Episode length: 207.90 +/- 18.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 208         |\n",
      "|    mean_reward          | 289         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 816000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004678901 |\n",
      "|    clip_fraction        | 0.0422      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.841       |\n",
      "|    n_updates            | 768         |\n",
      "|    policy_gradient_loss | 0.000942    |\n",
      "|    std                  | 0.811       |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 1204     |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 680      |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=226.63 +/- 93.42\n",
      "Episode length: 216.00 +/- 25.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 216          |\n",
      "|    mean_reward          | 227          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 832000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037581914 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.41        |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.853        |\n",
      "|    n_updates            | 772          |\n",
      "|    policy_gradient_loss | 0.00019      |\n",
      "|    std                  | 0.808        |\n",
      "|    value_loss           | 29.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 154      |\n",
      "| time/              |          |\n",
      "|    fps             | 1205     |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 693      |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=252.31 +/- 76.20\n",
      "Episode length: 196.50 +/- 16.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 196          |\n",
      "|    mean_reward          | 252          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 848000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036531696 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.4         |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.76         |\n",
      "|    n_updates            | 776          |\n",
      "|    policy_gradient_loss | -0.000532    |\n",
      "|    std                  | 0.808        |\n",
      "|    value_loss           | 25.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 1204     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 707      |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=277.53 +/- 15.65\n",
      "Episode length: 214.30 +/- 15.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 214         |\n",
      "|    mean_reward          | 278         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 864000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006305974 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.4        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.000681   |\n",
      "|    std                  | 0.802       |\n",
      "|    value_loss           | 55          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 154      |\n",
      "| time/              |          |\n",
      "|    fps             | 1205     |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 720      |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=265.34 +/- 63.36\n",
      "Episode length: 213.50 +/- 11.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 214         |\n",
      "|    mean_reward          | 265         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004002108 |\n",
      "|    clip_fraction        | 0.0368      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.54        |\n",
      "|    n_updates            | 784         |\n",
      "|    policy_gradient_loss | -3.65e-05   |\n",
      "|    std                  | 0.79        |\n",
      "|    value_loss           | 30.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 1207     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 732      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=279.06 +/- 22.13\n",
      "Episode length: 196.80 +/- 18.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 197          |\n",
      "|    mean_reward          | 279          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 896000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030664457 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.36        |\n",
      "|    explained_variance   | 0.968        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.5         |\n",
      "|    n_updates            | 788          |\n",
      "|    policy_gradient_loss | 0.000545     |\n",
      "|    std                  | 0.794        |\n",
      "|    value_loss           | 61.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 1209     |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 745      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=288.49 +/- 20.80\n",
      "Episode length: 210.80 +/- 13.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 211          |\n",
      "|    mean_reward          | 288          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 912000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019730167 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.38        |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.2         |\n",
      "|    n_updates            | 792          |\n",
      "|    policy_gradient_loss | -0.000894    |\n",
      "|    std                  | 0.799        |\n",
      "|    value_loss           | 75.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 148      |\n",
      "| time/              |          |\n",
      "|    fps             | 1212     |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 757      |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=287.10 +/- 21.68\n",
      "Episode length: 221.80 +/- 62.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 222          |\n",
      "|    mean_reward          | 287          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 928000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047908155 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.39        |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 796          |\n",
      "|    policy_gradient_loss | -0.000275    |\n",
      "|    std                  | 0.806        |\n",
      "|    value_loss           | 97.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 1213     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 769      |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=289.17 +/- 23.25\n",
      "Episode length: 198.10 +/- 15.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 198         |\n",
      "|    mean_reward          | 289         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 944000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004204544 |\n",
      "|    clip_fraction        | 0.0484      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | 0.00107     |\n",
      "|    std                  | 0.797       |\n",
      "|    value_loss           | 4.16        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 147      |\n",
      "| time/              |          |\n",
      "|    fps             | 1214     |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 782      |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=279.66 +/- 16.41\n",
      "Episode length: 204.20 +/- 12.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 204          |\n",
      "|    mean_reward          | 280          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 960000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026584095 |\n",
      "|    clip_fraction        | 0.0122       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.38        |\n",
      "|    explained_variance   | 0.942        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.2         |\n",
      "|    n_updates            | 804          |\n",
      "|    policy_gradient_loss | -0.000173    |\n",
      "|    std                  | 0.796        |\n",
      "|    value_loss           | 127          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 1216     |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 794      |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=251.10 +/- 68.83\n",
      "Episode length: 218.30 +/- 32.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 218          |\n",
      "|    mean_reward          | 251          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 976000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036946784 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.35        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.342        |\n",
      "|    n_updates            | 808          |\n",
      "|    policy_gradient_loss | -0.000599    |\n",
      "|    std                  | 0.783        |\n",
      "|    value_loss           | 2.19         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 1218     |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 807      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=285.95 +/- 17.20\n",
      "Episode length: 200.30 +/- 18.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 286         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 992000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004751769 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.35       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.532       |\n",
      "|    n_updates            | 812         |\n",
      "|    policy_gradient_loss | -0.000235   |\n",
      "|    std                  | 0.788       |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    fps             | 1219     |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 819      |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=278.32 +/- 17.22\n",
      "Episode length: 213.90 +/- 31.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 214          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1008000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051832525 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.35        |\n",
      "|    explained_variance   | 0.988        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.83         |\n",
      "|    n_updates            | 816          |\n",
      "|    policy_gradient_loss | 0.000313     |\n",
      "|    std                  | 0.784        |\n",
      "|    value_loss           | 15.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    fps             | 1219     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 833      |\n",
      "|    total_timesteps | 1015808  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x251203320a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e6), callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'data/policies/LunarLander{\"Continuous\" if continuous else \"\"}-v2#ppo#train_best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a094b89469df4117159496550a75e1ef6dfce4ed77bce29eb93efb1f0342af31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
