{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = make_vec_env(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2', n_envs=16)\n",
    "eval_env = Monitor(gym.make(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2'))\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo',\n",
    "    log_path=f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo',\n",
    "    eval_freq=1000,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(f'./logs/{\"continuous\" if continuous else \"discrete\"}/ppo/best_model', env=train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/ppo_lunarcontinuous_tensorboard/PPO_3\n",
      "Eval num_timesteps=16000, episode_reward=274.35 +/- 14.13\n",
      "Episode length: 260.00 +/- 11.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 260      |\n",
      "|    mean_reward     | 274      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    fps             | 929      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=281.91 +/- 13.18\n",
      "Episode length: 267.20 +/- 12.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 267         |\n",
      "|    mean_reward          | 282         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004793799 |\n",
      "|    clip_fraction        | 0.0488      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.829       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 0.000533    |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    fps             | 754      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=267.74 +/- 21.65\n",
      "Episode length: 272.00 +/- 18.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 272          |\n",
      "|    mean_reward          | 268          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038408197 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01         |\n",
      "|    n_updates            | 344          |\n",
      "|    policy_gradient_loss | -0.00049     |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 1.9          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 159      |\n",
      "| time/              |          |\n",
      "|    fps             | 716      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=280.91 +/- 21.39\n",
      "Episode length: 262.10 +/- 13.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 262          |\n",
      "|    mean_reward          | 281          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044124215 |\n",
      "|    clip_fraction        | 0.0454       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.272        |\n",
      "|    n_updates            | 348          |\n",
      "|    policy_gradient_loss | -3.37e-05    |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 1.85         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 987      |\n",
      "|    ep_rew_mean     | 158      |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=274.81 +/- 15.12\n",
      "Episode length: 269.40 +/- 11.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 269        |\n",
      "|    mean_reward          | 275        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00425729 |\n",
      "|    clip_fraction        | 0.041      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.78      |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 42.6       |\n",
      "|    n_updates            | 352        |\n",
      "|    policy_gradient_loss | -0.00153   |\n",
      "|    std                  | 0.98       |\n",
      "|    value_loss           | 12.8       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 980      |\n",
      "|    ep_rew_mean     | 158      |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=274.27 +/- 19.55\n",
      "Episode length: 271.20 +/- 15.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 271        |\n",
      "|    mean_reward          | 274        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 96000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00581008 |\n",
      "|    clip_fraction        | 0.0416     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.77      |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 66.5       |\n",
      "|    n_updates            | 356        |\n",
      "|    policy_gradient_loss | 0.000579   |\n",
      "|    std                  | 0.982      |\n",
      "|    value_loss           | 18.6       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 958      |\n",
      "|    ep_rew_mean     | 154      |\n",
      "| time/              |          |\n",
      "|    fps             | 692      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 141      |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=259.18 +/- 19.22\n",
      "Episode length: 264.40 +/- 19.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 264        |\n",
      "|    mean_reward          | 259        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 112000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00632196 |\n",
      "|    clip_fraction        | 0.0515     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.77      |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 48.5       |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | 5.86e-05   |\n",
      "|    std                  | 0.974      |\n",
      "|    value_loss           | 36         |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 959      |\n",
      "|    ep_rew_mean     | 154      |\n",
      "| time/              |          |\n",
      "|    fps             | 692      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 165      |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=276.70 +/- 15.74\n",
      "Episode length: 261.00 +/- 13.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 261        |\n",
      "|    mean_reward          | 277        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 128000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00497341 |\n",
      "|    clip_fraction        | 0.0522     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.75      |\n",
      "|    explained_variance   | 0.999      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.795      |\n",
      "|    n_updates            | 364        |\n",
      "|    policy_gradient_loss | -0.000344  |\n",
      "|    std                  | 0.963      |\n",
      "|    value_loss           | 1.43       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 951      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 690      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=273.99 +/- 21.01\n",
      "Episode length: 264.70 +/- 15.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 265          |\n",
      "|    mean_reward          | 274          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037112753 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.993        |\n",
      "|    n_updates            | 368          |\n",
      "|    policy_gradient_loss | -0.000154    |\n",
      "|    std                  | 0.957        |\n",
      "|    value_loss           | 17.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 935      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=278.18 +/- 17.61\n",
      "Episode length: 267.70 +/- 23.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 268        |\n",
      "|    mean_reward          | 278        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00394671 |\n",
      "|    clip_fraction        | 0.0195     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.72      |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.17       |\n",
      "|    n_updates            | 372        |\n",
      "|    policy_gradient_loss | 6.28e-05   |\n",
      "|    std                  | 0.961      |\n",
      "|    value_loss           | 38         |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 926      |\n",
      "|    ep_rew_mean     | 148      |\n",
      "| time/              |          |\n",
      "|    fps             | 660      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=274.46 +/- 19.44\n",
      "Episode length: 268.40 +/- 15.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 268          |\n",
      "|    mean_reward          | 274          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035243349 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.3         |\n",
      "|    n_updates            | 376          |\n",
      "|    policy_gradient_loss | -0.000301    |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 29.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 943      |\n",
      "|    ep_rew_mean     | 149      |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=265.08 +/- 19.73\n",
      "Episode length: 276.90 +/- 16.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 277         |\n",
      "|    mean_reward          | 265         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 192000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005756319 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.913       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | 1.84e-05    |\n",
      "|    std                  | 0.963       |\n",
      "|    value_loss           | 15.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 935      |\n",
      "|    ep_rew_mean     | 148      |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 304      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=281.04 +/- 23.53\n",
      "Episode length: 300.20 +/- 96.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 300          |\n",
      "|    mean_reward          | 281          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 208000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029252884 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37         |\n",
      "|    n_updates            | 384          |\n",
      "|    policy_gradient_loss | -0.000257    |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 14.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 935      |\n",
      "|    ep_rew_mean     | 149      |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 332      |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=278.50 +/- 21.33\n",
      "Episode length: 263.90 +/- 17.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 264          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041810293 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.233        |\n",
      "|    n_updates            | 388          |\n",
      "|    policy_gradient_loss | 0.000168     |\n",
      "|    std                  | 0.955        |\n",
      "|    value_loss           | 2.05         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 943      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 638      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 359      |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=279.89 +/- 22.67\n",
      "Episode length: 259.90 +/- 15.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 260          |\n",
      "|    mean_reward          | 280          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051775267 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.22         |\n",
      "|    n_updates            | 392          |\n",
      "|    policy_gradient_loss | 0.000669     |\n",
      "|    std                  | 0.957        |\n",
      "|    value_loss           | 12.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 960      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 389      |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=272.07 +/- 24.51\n",
      "Episode length: 267.50 +/- 17.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 268         |\n",
      "|    mean_reward          | 272         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004220385 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.345       |\n",
      "|    n_updates            | 396         |\n",
      "|    policy_gradient_loss | 0.000476    |\n",
      "|    std                  | 0.961       |\n",
      "|    value_loss           | 1.64        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 154      |\n",
      "| time/              |          |\n",
      "|    fps             | 626      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 418      |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=281.93 +/- 24.10\n",
      "Episode length: 266.80 +/- 20.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 267          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 272000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035067922 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01         |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | 0.000382     |\n",
      "|    std                  | 0.958        |\n",
      "|    value_loss           | 18           |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 951      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 624      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 446      |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=252.11 +/- 64.08\n",
      "Episode length: 255.30 +/- 34.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 255          |\n",
      "|    mean_reward          | 252          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 288000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035229689 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.969        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.3         |\n",
      "|    n_updates            | 404          |\n",
      "|    policy_gradient_loss | -6.02e-05    |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 49.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 951      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 622      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 473      |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=264.32 +/- 59.69\n",
      "Episode length: 249.70 +/- 22.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 250          |\n",
      "|    mean_reward          | 264          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 304000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036375052 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.275        |\n",
      "|    n_updates            | 408          |\n",
      "|    policy_gradient_loss | -0.000337    |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 1.91         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 622      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 500      |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=252.59 +/- 68.30\n",
      "Episode length: 252.10 +/- 23.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 252        |\n",
      "|    mean_reward          | 253        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 320000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00513738 |\n",
      "|    clip_fraction        | 0.033      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.71      |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.7        |\n",
      "|    n_updates            | 412        |\n",
      "|    policy_gradient_loss | -0.000336  |\n",
      "|    std                  | 0.957      |\n",
      "|    value_loss           | 26.9       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 934      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 620      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 527      |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=289.99 +/- 19.59\n",
      "Episode length: 270.40 +/- 17.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 270         |\n",
      "|    mean_reward          | 290         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004188318 |\n",
      "|    clip_fraction        | 0.0296      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.33        |\n",
      "|    n_updates            | 416         |\n",
      "|    policy_gradient_loss | 0.000962    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 15.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 926      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 619      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 555      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=245.91 +/- 81.22\n",
      "Episode length: 246.20 +/- 20.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 246          |\n",
      "|    mean_reward          | 246          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 352000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041075554 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.765        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | 0.000556     |\n",
      "|    std                  | 0.946        |\n",
      "|    value_loss           | 17.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 617      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 583      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=278.35 +/- 23.32\n",
      "Episode length: 257.00 +/- 13.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 257          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 368000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032645527 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.73         |\n",
      "|    n_updates            | 424          |\n",
      "|    policy_gradient_loss | -9.84e-05    |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 32.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    fps             | 617      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 610      |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=237.44 +/- 87.34\n",
      "Episode length: 240.10 +/- 41.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 240          |\n",
      "|    mean_reward          | 237          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 384000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030469687 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.9         |\n",
      "|    n_updates            | 428          |\n",
      "|    policy_gradient_loss | -0.000677    |\n",
      "|    std                  | 0.951        |\n",
      "|    value_loss           | 91.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 141      |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 635      |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=287.01 +/- 21.91\n",
      "Episode length: 255.00 +/- 23.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 255         |\n",
      "|    mean_reward          | 287         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003250779 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 432         |\n",
      "|    policy_gradient_loss | -0.000246   |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 35.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 136      |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 661      |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=284.17 +/- 27.08\n",
      "Episode length: 260.50 +/- 14.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 260          |\n",
      "|    mean_reward          | 284          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 416000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027927947 |\n",
      "|    clip_fraction        | 0.0159       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.952        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14           |\n",
      "|    n_updates            | 436          |\n",
      "|    policy_gradient_loss | -0.000189    |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 84.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 138      |\n",
      "| time/              |          |\n",
      "|    fps             | 617      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 689      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=217.14 +/- 96.44\n",
      "Episode length: 230.30 +/- 27.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 230         |\n",
      "|    mean_reward          | 217         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 432000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005802536 |\n",
      "|    clip_fraction        | 0.0556      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.94        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.00124     |\n",
      "|    std                  | 0.959       |\n",
      "|    value_loss           | 19.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 135      |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 717      |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=271.99 +/- 22.82\n",
      "Episode length: 252.90 +/- 9.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 253          |\n",
      "|    mean_reward          | 272          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 448000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039011687 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.963        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.1          |\n",
      "|    n_updates            | 444          |\n",
      "|    policy_gradient_loss | 0.000228     |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 48.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 141      |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 744      |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=284.24 +/- 20.58\n",
      "Episode length: 251.80 +/- 15.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 252          |\n",
      "|    mean_reward          | 284          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 464000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047657867 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.949        |\n",
      "|    n_updates            | 448          |\n",
      "|    policy_gradient_loss | 0.000488     |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 3.31         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 771      |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=252.61 +/- 76.50\n",
      "Episode length: 241.50 +/- 27.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 242         |\n",
      "|    mean_reward          | 253         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003181513 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.69        |\n",
      "|    n_updates            | 452         |\n",
      "|    policy_gradient_loss | 0.000162    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 47.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 148      |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 797      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=280.84 +/- 20.95\n",
      "Episode length: 241.20 +/- 14.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 241          |\n",
      "|    mean_reward          | 281          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 496000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032992023 |\n",
      "|    clip_fraction        | 0.0268       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.969        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.8         |\n",
      "|    n_updates            | 456          |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 59.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 147      |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 823      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=282.09 +/- 18.63\n",
      "Episode length: 250.80 +/- 21.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033766776 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000616    |\n",
      "|    std                  | 0.957        |\n",
      "|    value_loss           | 48.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 850      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=275.52 +/- 21.26\n",
      "Episode length: 245.10 +/- 17.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 245         |\n",
      "|    mean_reward          | 276         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 528000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004218401 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.766       |\n",
      "|    n_updates            | 464         |\n",
      "|    policy_gradient_loss | 0.000959    |\n",
      "|    std                  | 0.964       |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 615      |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 878      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=269.12 +/- 23.82\n",
      "Episode length: 248.80 +/- 23.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 249          |\n",
      "|    mean_reward          | 269          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043897517 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45         |\n",
      "|    n_updates            | 468          |\n",
      "|    policy_gradient_loss | -0.000406    |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 16.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 615      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 905      |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=272.04 +/- 16.25\n",
      "Episode length: 316.30 +/- 57.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 316          |\n",
      "|    mean_reward          | 272          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 560000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028162678 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.99         |\n",
      "|    n_updates            | 472          |\n",
      "|    policy_gradient_loss | -0.000635    |\n",
      "|    std                  | 0.951        |\n",
      "|    value_loss           | 30.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 612      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 936      |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=278.23 +/- 17.23\n",
      "Episode length: 313.30 +/- 93.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 313          |\n",
      "|    mean_reward          | 278          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 576000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039947648 |\n",
      "|    clip_fraction        | 0.0439       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.306        |\n",
      "|    n_updates            | 476          |\n",
      "|    policy_gradient_loss | 0.000127     |\n",
      "|    std                  | 0.938        |\n",
      "|    value_loss           | 1.85         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    fps             | 609      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 967      |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=278.82 +/- 20.95\n",
      "Episode length: 252.60 +/- 23.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 253        |\n",
      "|    mean_reward          | 279        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 592000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00563256 |\n",
      "|    clip_fraction        | 0.0513     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.67      |\n",
      "|    explained_variance   | 0.999      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.333      |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | 0.000138   |\n",
      "|    std                  | 0.937      |\n",
      "|    value_loss           | 1.17       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 975      |\n",
      "|    ep_rew_mean     | 159      |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 995      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=279.10 +/- 19.45\n",
      "Episode length: 268.70 +/- 39.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 269          |\n",
      "|    mean_reward          | 279          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 608000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048564374 |\n",
      "|    clip_fraction        | 0.0424       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.417        |\n",
      "|    n_updates            | 484          |\n",
      "|    policy_gradient_loss | 2.13e-05     |\n",
      "|    std                  | 0.939        |\n",
      "|    value_loss           | 1.31         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 959      |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 1026     |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=281.83 +/- 18.45\n",
      "Episode length: 287.60 +/- 79.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 288         |\n",
      "|    mean_reward          | 282         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 624000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004222492 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.05        |\n",
      "|    n_updates            | 488         |\n",
      "|    policy_gradient_loss | 0.000123    |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 23.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 1055     |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=276.40 +/- 14.04\n",
      "Episode length: 250.00 +/- 19.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 250        |\n",
      "|    mean_reward          | 276        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 640000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00504377 |\n",
      "|    clip_fraction        | 0.0417     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.68      |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.97       |\n",
      "|    n_updates            | 492        |\n",
      "|    policy_gradient_loss | -0.00171   |\n",
      "|    std                  | 0.939      |\n",
      "|    value_loss           | 18.7       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 975      |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 1082     |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=271.85 +/- 19.28\n",
      "Episode length: 243.10 +/- 14.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 243          |\n",
      "|    mean_reward          | 272          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 656000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054979534 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.672        |\n",
      "|    n_updates            | 496          |\n",
      "|    policy_gradient_loss | 0.000189     |\n",
      "|    std                  | 0.932        |\n",
      "|    value_loss           | 1.28         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 975      |\n",
      "|    ep_rew_mean     | 164      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 1110     |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=292.60 +/- 16.20\n",
      "Episode length: 255.00 +/- 19.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 255         |\n",
      "|    mean_reward          | 293         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 672000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004615234 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.372       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.000291   |\n",
      "|    std                  | 0.933       |\n",
      "|    value_loss           | 0.995       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=688000, episode_reward=272.72 +/- 18.26\n",
      "Episode length: 250.90 +/- 23.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 251      |\n",
      "|    mean_reward     | 273      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 951      |\n",
      "|    ep_rew_mean     | 162      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 1139     |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=273.85 +/- 22.81\n",
      "Episode length: 236.00 +/- 11.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 236          |\n",
      "|    mean_reward          | 274          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 704000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027791434 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.975        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.17         |\n",
      "|    n_updates            | 504          |\n",
      "|    policy_gradient_loss | -0.000664    |\n",
      "|    std                  | 0.928        |\n",
      "|    value_loss           | 46.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 951      |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 1165     |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=284.01 +/- 19.16\n",
      "Episode length: 240.90 +/- 16.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 241         |\n",
      "|    mean_reward          | 284         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005833714 |\n",
      "|    clip_fraction        | 0.0451      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.296       |\n",
      "|    n_updates            | 508         |\n",
      "|    policy_gradient_loss | 0.00024     |\n",
      "|    std                  | 0.926       |\n",
      "|    value_loss           | 14.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 959      |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 1192     |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=285.53 +/- 13.54\n",
      "Episode length: 228.90 +/- 13.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 229          |\n",
      "|    mean_reward          | 286          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 736000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041462444 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.388        |\n",
      "|    n_updates            | 512          |\n",
      "|    policy_gradient_loss | -0.000824    |\n",
      "|    std                  | 0.921        |\n",
      "|    value_loss           | 0.953        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 958      |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 1220     |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=274.98 +/- 18.23\n",
      "Episode length: 237.10 +/- 12.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 237          |\n",
      "|    mean_reward          | 275          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 752000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044514136 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.278        |\n",
      "|    n_updates            | 516          |\n",
      "|    policy_gradient_loss | 5.47e-05     |\n",
      "|    std                  | 0.916        |\n",
      "|    value_loss           | 20.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 1246     |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=274.32 +/- 13.92\n",
      "Episode length: 236.10 +/- 17.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 236         |\n",
      "|    mean_reward          | 274         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 768000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005941863 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.749       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.000143   |\n",
      "|    std                  | 0.918       |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 933      |\n",
      "|    ep_rew_mean     | 160      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 1272     |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=274.87 +/- 17.46\n",
      "Episode length: 240.90 +/- 21.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 241         |\n",
      "|    mean_reward          | 275         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 784000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003955896 |\n",
      "|    clip_fraction        | 0.0265      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.4        |\n",
      "|    n_updates            | 524         |\n",
      "|    policy_gradient_loss | -0.000378   |\n",
      "|    std                  | 0.915       |\n",
      "|    value_loss           | 34.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 949      |\n",
      "|    ep_rew_mean     | 159      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 1299     |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=284.54 +/- 16.39\n",
      "Episode length: 238.30 +/- 16.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 238         |\n",
      "|    mean_reward          | 285         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005812628 |\n",
      "|    clip_fraction        | 0.0561      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.38        |\n",
      "|    n_updates            | 528         |\n",
      "|    policy_gradient_loss | 0.000281    |\n",
      "|    std                  | 0.906       |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 941      |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 1328     |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=280.90 +/- 15.67\n",
      "Episode length: 241.90 +/- 18.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 242         |\n",
      "|    mean_reward          | 281         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 816000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003846396 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.989       |\n",
      "|    n_updates            | 532         |\n",
      "|    policy_gradient_loss | 0.000208    |\n",
      "|    std                  | 0.903       |\n",
      "|    value_loss           | 16.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 933      |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 1354     |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=276.00 +/- 23.87\n",
      "Episode length: 236.60 +/- 16.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 237         |\n",
      "|    mean_reward          | 276         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 832000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004141464 |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.6        |\n",
      "|    n_updates            | 536         |\n",
      "|    policy_gradient_loss | -0.000381   |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 18.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 1381     |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=267.74 +/- 19.26\n",
      "Episode length: 241.20 +/- 21.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 241          |\n",
      "|    mean_reward          | 268          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 848000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045194873 |\n",
      "|    clip_fraction        | 0.0524       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.333        |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -7.65e-05    |\n",
      "|    std                  | 0.908        |\n",
      "|    value_loss           | 16.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 1406     |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=283.49 +/- 20.94\n",
      "Episode length: 248.20 +/- 22.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 248          |\n",
      "|    mean_reward          | 283          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 864000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050002323 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.78         |\n",
      "|    n_updates            | 544          |\n",
      "|    policy_gradient_loss | -5.06e-05    |\n",
      "|    std                  | 0.902        |\n",
      "|    value_loss           | 9.91         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 959      |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 1434     |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=281.53 +/- 23.17\n",
      "Episode length: 237.00 +/- 18.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 237         |\n",
      "|    mean_reward          | 282         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003630307 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.28        |\n",
      "|    n_updates            | 548         |\n",
      "|    policy_gradient_loss | 0.000365    |\n",
      "|    std                  | 0.899       |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 959      |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 1462     |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=275.21 +/- 14.52\n",
      "Episode length: 238.30 +/- 15.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 238          |\n",
      "|    mean_reward          | 275          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 896000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038900012 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18         |\n",
      "|    n_updates            | 552          |\n",
      "|    policy_gradient_loss | -0.000491    |\n",
      "|    std                  | 0.898        |\n",
      "|    value_loss           | 19.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 958      |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 1489     |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=282.17 +/- 17.94\n",
      "Episode length: 232.80 +/- 11.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 233          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 912000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048962454 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.9         |\n",
      "|    n_updates            | 556          |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    std                  | 0.9          |\n",
      "|    value_loss           | 24.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 966      |\n",
      "|    ep_rew_mean     | 158      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 1516     |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=283.47 +/- 11.28\n",
      "Episode length: 232.50 +/- 19.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 232          |\n",
      "|    mean_reward          | 283          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 928000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044720033 |\n",
      "|    clip_fraction        | 0.0475       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.356        |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | 0.000194     |\n",
      "|    std                  | 0.902        |\n",
      "|    value_loss           | 0.882        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 958      |\n",
      "|    ep_rew_mean     | 158      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 1544     |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=278.26 +/- 19.68\n",
      "Episode length: 240.40 +/- 19.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 240         |\n",
      "|    mean_reward          | 278         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 944000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002825662 |\n",
      "|    clip_fraction        | 0.0165      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 564         |\n",
      "|    policy_gradient_loss | 0.000432    |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 28.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 1572     |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=276.22 +/- 17.81\n",
      "Episode length: 232.30 +/- 11.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 232         |\n",
      "|    mean_reward          | 276         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004235723 |\n",
      "|    clip_fraction        | 0.0172      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.47        |\n",
      "|    n_updates            | 568         |\n",
      "|    policy_gradient_loss | -0.000517   |\n",
      "|    std                  | 0.904       |\n",
      "|    value_loss           | 68.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 1599     |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=270.71 +/- 23.39\n",
      "Episode length: 230.80 +/- 16.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 231          |\n",
      "|    mean_reward          | 271          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 976000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048540737 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3          |\n",
      "|    n_updates            | 572          |\n",
      "|    policy_gradient_loss | 3.41e-05     |\n",
      "|    std                  | 0.899        |\n",
      "|    value_loss           | 17.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 1627     |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=272.04 +/- 18.48\n",
      "Episode length: 248.70 +/- 22.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 249          |\n",
      "|    mean_reward          | 272          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 992000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021983762 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 576          |\n",
      "|    policy_gradient_loss | -0.000574    |\n",
      "|    std                  | 0.9          |\n",
      "|    value_loss           | 53.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 145      |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 1656     |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=271.79 +/- 24.38\n",
      "Episode length: 238.90 +/- 15.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 239          |\n",
      "|    mean_reward          | 272          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1008000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030550775 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.972        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.12         |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.000272    |\n",
      "|    std                  | 0.901        |\n",
      "|    value_loss           | 46.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 832      |\n",
      "|    ep_rew_mean     | 142      |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 1683     |\n",
      "|    total_timesteps | 1015808  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1dbd5a85d20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e6), callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel sest bloqu lors de lexcution du code dans la cellule active ou une cellule prcdente. Veuillez vrifier le code dans la ou les cellules pour identifier une cause possible de lchec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus dinformations. Pour plus dinformations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "model.save(f'data/policies/LunarLander{\"Continuous\" if continuous else \"\"}-v2#ppo#train_best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
