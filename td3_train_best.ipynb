{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "eval_env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/continuous/td3\",\n",
    "    log_path=\"./logs/continuous/td3\",\n",
    "    eval_freq=1000,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = TD3.load(\"logs/continuous/td3/best_model\", env=train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/td3_lunar_tensorboard/TD3_4\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 117      |\n",
      "|    ep_rew_mean     | -140     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 3286     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 468      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jules Dubreuil\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=175.04 +/- 53.30\n",
      "Episode length: 544.60 +/- 231.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 545      |\n",
      "|    mean_reward     | 175      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 116      |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 924      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=170.63 +/- 90.03\n",
      "Episode length: 368.00 +/- 59.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 368      |\n",
      "|    mean_reward     | 171      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | -233     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 1300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=176.70 +/- 57.56\n",
      "Episode length: 508.00 +/- 116.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 508      |\n",
      "|    mean_reward     | 177      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 125      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 1742     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=158.96 +/- 170.89\n",
      "Episode length: 462.00 +/- 149.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 462      |\n",
      "|    mean_reward     | 159      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 116      |\n",
      "|    ep_rew_mean     | -274     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 128      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 2322     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=203.11 +/- 40.45\n",
      "Episode length: 500.60 +/- 201.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 501      |\n",
      "|    mean_reward     | 203      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -264     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 2711     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=156.01 +/- 82.15\n",
      "Episode length: 632.80 +/- 241.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 633      |\n",
      "|    mean_reward     | 156      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 106      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 3184     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=182.57 +/- 88.73\n",
      "Episode length: 526.80 +/- 245.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 527      |\n",
      "|    mean_reward     | 183      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 103      |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 3615     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=146.71 +/- 165.48\n",
      "Episode length: 446.20 +/- 39.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 446      |\n",
      "|    mean_reward     | 147      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -240     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 106      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 4064     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=220.81 +/- 22.32\n",
      "Episode length: 354.80 +/- 74.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 355      |\n",
      "|    mean_reward     | 221      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -234     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 112      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 4559     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=105.11 +/- 102.12\n",
      "Episode length: 362.00 +/- 111.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 105      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -234     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 113      |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 5010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=16.89 +/- 382.02\n",
      "Episode length: 412.40 +/- 77.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 412      |\n",
      "|    mean_reward     | 16.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 115      |\n",
      "|    ep_rew_mean     | -240     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 115      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 5510     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -239     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 123      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 5903     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=109.84 +/- 131.45\n",
      "Episode length: 394.00 +/- 93.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 394      |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 115      |\n",
      "|    ep_rew_mean     | -240     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 128      |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 6464     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=132.66 +/- 188.14\n",
      "Episode length: 458.60 +/- 74.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 459      |\n",
      "|    mean_reward     | 133      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -244     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 6860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=239.86 +/- 39.30\n",
      "Episode length: 378.00 +/- 27.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 378      |\n",
      "|    mean_reward     | 240      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -244     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 7321     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=256.12 +/- 20.77\n",
      "Episode length: 344.20 +/- 70.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 344      |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -238     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 7709     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=104.18 +/- 215.02\n",
      "Episode length: 382.80 +/- 149.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -237     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 8178     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=227.34 +/- 39.02\n",
      "Episode length: 433.60 +/- 121.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 434      |\n",
      "|    mean_reward     | 227      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -236     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 8595     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=244.25 +/- 19.88\n",
      "Episode length: 349.40 +/- 42.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 349      |\n",
      "|    mean_reward     | 244      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -233     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 133      |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 9063     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=42.68 +/- 255.48\n",
      "Episode length: 533.60 +/- 180.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 534      |\n",
      "|    mean_reward     | 42.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -232     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 9541     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=182.53 +/- 22.96\n",
      "Episode length: 545.60 +/- 217.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 546      |\n",
      "|    mean_reward     | 183      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -231     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 10024    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=215.65 +/- 27.03\n",
      "Episode length: 391.60 +/- 62.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 392      |\n",
      "|    mean_reward     | 216      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.36    |\n",
      "|    critic_loss     | 41.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 35609    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-153.59 +/- 240.02\n",
      "Episode length: 752.80 +/- 247.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 753      |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.65    |\n",
      "|    critic_loss     | 38.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 36082    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=196.94 +/- 52.16\n",
      "Episode length: 667.20 +/- 204.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | 197      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.96    |\n",
      "|    critic_loss     | 30.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 36372    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 109      |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 11580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=110.10 +/- 160.35\n",
      "Episode length: 395.20 +/- 161.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 395      |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.25    |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 36881    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=-9.41 +/- 141.48\n",
      "Episode length: 620.40 +/- 162.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 620      |\n",
      "|    mean_reward     | -9.41    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12500    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.83    |\n",
      "|    critic_loss     | 32.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 37791    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39m1e6\u001b[39m), callback\u001b[39m=\u001b[39meval_callback)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m: TD3Self,\n\u001b[0;32m    211\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    220\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TD3Self:\n\u001b[1;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    223\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    224\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    225\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    226\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    227\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    228\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    229\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    230\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    231\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    232\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:350\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    347\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    349\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 350\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[0;32m    351\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[0;32m    352\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[0;32m    353\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[0;32m    354\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    355\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[0;32m    356\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[0;32m    357\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    360\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:591\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    589\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m    590\u001b[0m \u001b[39m# Only stop training if return value is False, not when it is None.\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    592\u001b[0m     \u001b[39mreturn\u001b[39;00m RolloutReturn(num_collected_steps \u001b[39m*\u001b[39m env\u001b[39m.\u001b[39mnum_envs, num_collected_episodes, continue_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    594\u001b[0m \u001b[39m# Retrieve reward and episode length if using Monitor wrapper\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\callbacks.py:88\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39m# timesteps start at zero\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[1;32m---> 88\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\callbacks.py:435\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39m# Reset success rate buffer\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_success_buffer \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 435\u001b[0m episode_rewards, episode_lengths \u001b[39m=\u001b[39m evaluate_policy(\n\u001b[0;32m    436\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[0;32m    437\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_env,\n\u001b[0;32m    438\u001b[0m     n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    439\u001b[0m     render\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender,\n\u001b[0;32m    440\u001b[0m     deterministic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeterministic,\n\u001b[0;32m    441\u001b[0m     return_episode_rewards\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    442\u001b[0m     warn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn,\n\u001b[0;32m    443\u001b[0m     callback\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_success_callback,\n\u001b[0;32m    444\u001b[0m )\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluations_timesteps\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\evaluation.py:87\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[0;32m     86\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(observations, state\u001b[39m=\u001b[39mstates, episode_start\u001b[39m=\u001b[39mepisode_starts, deterministic\u001b[39m=\u001b[39mdeterministic)\n\u001b[1;32m---> 87\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m     88\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[0;32m     89\u001b[0m     current_lengths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:51\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m         obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m---> 51\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf(), np\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews), np\u001b[39m.\u001b[39;49mcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuf_dones), deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:959\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(a, order, subok)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_copy_dispatcher)\n\u001b[0;32m    871\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy\u001b[39m(a, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    872\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[39m    Return an array copy of the given object.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    957\u001b[0m \n\u001b[0;32m    958\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39mreturn\u001b[39;00m array(a, order\u001b[39m=\u001b[39;49morder, subok\u001b[39m=\u001b[39;49msubok, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e6), callback=eval_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
