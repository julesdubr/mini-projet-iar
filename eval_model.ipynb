{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [],
         "source": [
            "import gym\n",
            "\n",
            "from stable_baselines3 import DQN, PPO, TD3, SAC\n",
            "from stable_baselines3.common.evaluation import evaluate_policy\n",
            "from sb3_contrib import QRDQN, TQC\n",
            "\n",
            "ALGOS = {\n",
            "    \"dqn\": DQN,\n",
            "    \"ppo\": PPO,\n",
            "    \"sac\": SAC,\n",
            "    \"td3\": TD3,\n",
            "    \"qrdqn\": QRDQN,\n",
            "    \"tqc\": TQC,\n",
            "}\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [],
         "source": [
            "algo = \"ppo\"\n",
            "continuous = True\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Wrapping the env with a `Monitor` wrapper\n",
                  "Wrapping the env in a DummyVecEnv.\n"
               ]
            }
         ],
         "source": [
            "env = gym.make(f'LunarLander{\"Continuous\" if continuous else \"\"}-v2')\n",
            "model = ALGOS[algo].load(f'logs/{\"continuous\" if continuous else \"discrete\"}/{algo}/best_model', env=env)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(283.5354851555556, 29.78254240953609)"
                  ]
               },
               "execution_count": 28,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=900)\n",
            "mean_reward, std_reward\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 29,
         "metadata": {},
         "outputs": [],
         "source": [
            "obs = env.reset()\n",
            "for i in range(500):\n",
            "    action, _ = model.predict(obs, deterministic=True)\n",
            "    obs, reward, done, info = env.step(action)\n",
            "    env.render()\n",
            "env.close()\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.10.5 64-bit",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.5"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}