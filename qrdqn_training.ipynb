{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from sb3_contrib import QRDQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make(f'LunarLander-v2')\n",
    "eval_env = gym.make(f'LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = QRDQN(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=6.3e-4,\n",
    "    batch_size=128,\n",
    "    buffer_size=50000,\n",
    "    gamma=0.99,\n",
    "    learning_starts=0,\n",
    "    target_update_interval=250,\n",
    "    train_freq=4,\n",
    "    gradient_steps=-1,\n",
    "    exploration_fraction=0.12,\n",
    "    exploration_final_eps=0.1,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    verbose=1,\n",
    "    tensorboard_log=f'./runs/qrdqn_lunar_tensorboard/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'./logs/discrete/qrdqn',\n",
    "    log_path=f'./logs/discrete/qrdqn',\n",
    "    eval_freq=1000,\n",
    "    n_eval_episodes=10,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/qrdqn_lunar_tensorboard/QRDQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.2     |\n",
      "|    ep_rew_mean      | -196     |\n",
      "|    exploration_rate | 0.972    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 369      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 184      |\n",
      "|    n_updates        | 368      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 83.9     |\n",
      "|    ep_rew_mean      | -154     |\n",
      "|    exploration_rate | 0.95     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 671      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 171      |\n",
      "|    n_updates        | 668      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tdubr\\documents\\jules\\stable-baselines3\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-204.35 +/- 45.19\n",
      "Episode length: 953.30 +/- 140.10\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 953      |\n",
      "|    mean_reward      | -204     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.925    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 175      |\n",
      "|    n_updates        | 996      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 86.4     |\n",
      "|    ep_rew_mean      | -162     |\n",
      "|    exploration_rate | 0.922    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 1037     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 210      |\n",
      "|    n_updates        | 1036     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.2     |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration_rate | 0.889    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 1475     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 279      |\n",
      "|    n_updates        | 1472     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.6     |\n",
      "|    ep_rew_mean      | -142     |\n",
      "|    exploration_rate | 0.861    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 158      |\n",
      "|    total_timesteps  | 1852     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 309      |\n",
      "|    n_updates        | 1848     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-99.97 +/- 23.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -100     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.85     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 344      |\n",
      "|    n_updates        | 1996     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.1     |\n",
      "|    ep_rew_mean      | -142     |\n",
      "|    exploration_rate | 0.831    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total_timesteps  | 2259     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 388      |\n",
      "|    n_updates        | 2256     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.7     |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.799    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 234      |\n",
      "|    total_timesteps  | 2679     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 406      |\n",
      "|    n_updates        | 2676     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-161.61 +/- 50.49\n",
      "Episode length: 938.70 +/- 78.57\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 939      |\n",
      "|    mean_reward      | -162     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.775    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 3000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 416      |\n",
      "|    n_updates        | 2996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.3     |\n",
      "|    ep_rew_mean      | -133     |\n",
      "|    exploration_rate | 0.771    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total_timesteps  | 3051     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 454      |\n",
      "|    n_updates        | 3048     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97       |\n",
      "|    ep_rew_mean      | -134     |\n",
      "|    exploration_rate | 0.738    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 308      |\n",
      "|    total_timesteps  | 3493     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 522      |\n",
      "|    n_updates        | 3492     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.6     |\n",
      "|    ep_rew_mean      | -128     |\n",
      "|    exploration_rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 346      |\n",
      "|    total_timesteps  | 3983     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 449      |\n",
      "|    n_updates        | 3980     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-207.51 +/- 26.67\n",
      "Episode length: 530.50 +/- 79.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 530      |\n",
      "|    mean_reward      | -208     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.7      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 501      |\n",
      "|    n_updates        | 3996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | -121     |\n",
      "|    exploration_rate | 0.667    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 385      |\n",
      "|    total_timesteps  | 4439     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 488      |\n",
      "|    n_updates        | 4436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -115     |\n",
      "|    exploration_rate | 0.634    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 420      |\n",
      "|    total_timesteps  | 4885     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 531      |\n",
      "|    n_updates        | 4884     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-244.56 +/- 35.64\n",
      "Episode length: 551.10 +/- 146.65\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 551      |\n",
      "|    mean_reward      | -245     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.625    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 501      |\n",
      "|    n_updates        | 4996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -113     |\n",
      "|    exploration_rate | 0.592    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 467      |\n",
      "|    total_timesteps  | 5440     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 467      |\n",
      "|    n_updates        | 5436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.561    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 500      |\n",
      "|    total_timesteps  | 5854     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 529      |\n",
      "|    n_updates        | 5852     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-174.09 +/- 46.84\n",
      "Episode length: 844.70 +/- 265.73\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 845      |\n",
      "|    mean_reward      | -174     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.55     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 504      |\n",
      "|    n_updates        | 5996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | -106     |\n",
      "|    exploration_rate | 0.513    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 559      |\n",
      "|    total_timesteps  | 6497     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 551      |\n",
      "|    n_updates        | 6496     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-178.75 +/- 63.74\n",
      "Episode length: 888.50 +/- 176.67\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 888      |\n",
      "|    mean_reward      | -179     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.475    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 7000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 493      |\n",
      "|    n_updates        | 6996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 114      |\n",
      "|    ep_rew_mean      | -104     |\n",
      "|    exploration_rate | 0.452    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 632      |\n",
      "|    total_timesteps  | 7309     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 574      |\n",
      "|    n_updates        | 7308     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 114      |\n",
      "|    ep_rew_mean      | -104     |\n",
      "|    exploration_rate | 0.419    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 666      |\n",
      "|    total_timesteps  | 7746     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 536      |\n",
      "|    n_updates        | 7744     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-305.56 +/- 155.07\n",
      "Episode length: 828.40 +/- 121.32\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 828      |\n",
      "|    mean_reward      | -306     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.4      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 588      |\n",
      "|    n_updates        | 7996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 115      |\n",
      "|    ep_rew_mean      | -102     |\n",
      "|    exploration_rate | 0.378    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 717      |\n",
      "|    total_timesteps  | 8295     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 652      |\n",
      "|    n_updates        | 8292     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-356.95 +/- 321.93\n",
      "Episode length: 606.30 +/- 254.42\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 606      |\n",
      "|    mean_reward      | -357     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.325    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 9000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 576      |\n",
      "|    n_updates        | 8996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 123      |\n",
      "|    ep_rew_mean      | -101     |\n",
      "|    exploration_rate | 0.3      |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 804      |\n",
      "|    total_timesteps  | 9339     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 605      |\n",
      "|    n_updates        | 9336     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-150.60 +/- 76.62\n",
      "Episode length: 416.00 +/- 185.44\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 416      |\n",
      "|    mean_reward      | -151     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.25     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 619      |\n",
      "|    n_updates        | 9996     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-176.49 +/- 108.02\n",
      "Episode length: 769.20 +/- 201.66\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 769      |\n",
      "|    mean_reward      | -176     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.175    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 11000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 634      |\n",
      "|    n_updates        | 10996    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-51.98 +/- 74.57\n",
      "Episode length: 898.70 +/- 196.13\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 899      |\n",
      "|    mean_reward      | -52      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 567      |\n",
      "|    n_updates        | 11996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-119.10 +/- 70.97\n",
      "Episode length: 948.90 +/- 135.29\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 949      |\n",
      "|    mean_reward      | -119     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 13000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 558      |\n",
      "|    n_updates        | 12996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 166      |\n",
      "|    ep_rew_mean      | -100     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 1148     |\n",
      "|    total_timesteps  | 13258    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 599      |\n",
      "|    n_updates        | 13256    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-137.35 +/- 39.58\n",
      "Episode length: 506.80 +/- 277.09\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 507      |\n",
      "|    mean_reward      | -137     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 14000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 460      |\n",
      "|    n_updates        | 13996    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-57.15 +/- 21.18\n",
      "Episode length: 755.30 +/- 373.98\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 755      |\n",
      "|    mean_reward      | -57.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 516      |\n",
      "|    n_updates        | 14996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 185      |\n",
      "|    ep_rew_mean      | -101     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 1348     |\n",
      "|    total_timesteps  | 15558    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 502      |\n",
      "|    n_updates        | 15556    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-77.49 +/- 30.51\n",
      "Episode length: 459.90 +/- 378.01\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 460      |\n",
      "|    mean_reward      | -77.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 16000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 444      |\n",
      "|    n_updates        | 15996    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-43.18 +/- 40.46\n",
      "Episode length: 717.90 +/- 367.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 718      |\n",
      "|    mean_reward      | -43.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 17000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 406      |\n",
      "|    n_updates        | 16996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 203      |\n",
      "|    ep_rew_mean      | -99.6    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 1546     |\n",
      "|    total_timesteps  | 17883    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 453      |\n",
      "|    n_updates        | 17880    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-26.30 +/- 17.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -26.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 461      |\n",
      "|    n_updates        | 17996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=-23.43 +/- 11.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -23.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 19000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 445      |\n",
      "|    n_updates        | 18996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-36.25 +/- 18.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -36.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 428      |\n",
      "|    n_updates        | 19996    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=9.13 +/- 84.30\n",
      "Episode length: 965.40 +/- 103.80\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 965      |\n",
      "|    mean_reward      | 9.13     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 21000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 347      |\n",
      "|    n_updates        | 20996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 238      |\n",
      "|    ep_rew_mean      | -95.5    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 1913     |\n",
      "|    total_timesteps  | 21883    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 331      |\n",
      "|    n_updates        | 21880    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-38.38 +/- 14.81\n",
      "Episode length: 831.70 +/- 336.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 832      |\n",
      "|    mean_reward      | -38.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 22000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 378      |\n",
      "|    n_updates        | 21996    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=5.07 +/- 88.93\n",
      "Episode length: 838.70 +/- 323.16\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 839      |\n",
      "|    mean_reward      | 5.07     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 23000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 388      |\n",
      "|    n_updates        | 22996    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tdubr\\Documents\\jules\\mini-projet-iar\\qrdqn_training.ipynb Cellule 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tdubr/Documents/jules/mini-projet-iar/qrdqn_training.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(\u001b[39m1e5\u001b[39;49m), callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[1;32mc:\\Users\\tdubr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sb3_contrib\\qrdqn\\qrdqn.py:264\u001b[0m, in \u001b[0;36mQRDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    252\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    253\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    262\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    265\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    266\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    267\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    268\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    269\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    270\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    272\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    273\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\users\\tdubr\\documents\\jules\\stable-baselines3\\stable_baselines3\\common\\off_policy_algorithm.py:373\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    372\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 373\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[0;32m    375\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    377\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tdubr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sb3_contrib\\qrdqn\\qrdqn.py:206\u001b[0m, in \u001b[0;36mQRDQN.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m# Optimize the policy\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 206\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    207\u001b[0m \u001b[39m# Clip gradient norm\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tdubr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\tdubr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e5), callback=eval_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a094b89469df4117159496550a75e1ef6dfce4ed77bce29eb93efb1f0342af31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
